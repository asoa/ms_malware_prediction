{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary of Project\n",
    "* This notebook models data from https://www.kaggle.com/c/microsoft-malware-prediction/overview\n",
    "* Random Forest model generalizes better than the Logistic Regression with regularization; belive this is because of how bagging minimizes variance by aggregating weak predictors (trees) into a strong predictor\n",
    "* Believe the RF score can be improved by increasing the hashed feature space up from 200 (pyspark.ml.feature.FeatureHasher), grid-searching over the RF paramaters (numTrees/maxDepth) at the expense of higher computation costs and risk of over-fitting, and by removing features that don't have a relationship with the target -- using chi squared analyis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "---\n",
    "* [ ] analyze correlation of features using chi-squared statistic\n",
    "* [ ] reduce cardinality of features using frequency|binary encoding\n",
    "* [ ] grid search LR model regParam and elasticNet parameters \n",
    "* [ ] create random forest model\n",
    "* [ ] grid search RF model\n",
    "* [ ] create gradient-boosted tree models\n",
    "* [x] enable Adaptive Query Execution to handle data skew, shuffle, and join logic\n",
    "* [x] fix driver.maxResultSize error\n",
    "* [ ] create model with only features that show a relationship with target; using chi square statistic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment setup\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.conf import SparkConf\n",
    "from pyspark.storagelevel import StorageLevel\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.ml import feature, evaluation, Pipeline\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = (SparkConf()\n",
    "            .setAppName('ms_malware_pred')\n",
    "            .setMaster('spark://spark-master:7077')\n",
    "       )\n",
    "conf.set(\"spark.executor.cores\", \"2\")\n",
    "conf.set(\"spark.executor.memory\", \"16g\")\n",
    "conf.set(\"spark.driver.maxResultSize\", \"1g\")\n",
    "conf.set(\"spark.default.parallelism\", \"6\")  # default number of cores in application\n",
    "conf.set(\"spark.driver.memory\", \"3g\") \n",
    "conf.set(\"spark.shuffle.file.buffer\", \"1m\")\n",
    "conf.set(\"spark.shuffle.unsafe.file.output.buffer\", \"1m\")\n",
    "conf.set(\"spark.sql.shuffle.partitions\", \"18\")  # 3 * cores available (6)\n",
    "conf.set(\"spark.sql.adaptive.enabled\", \"true\") # enable Adaptive Query Execution\n",
    "conf.set(\"spark.kryoserializer.buffer.max\", \"1g\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext.getConf().getAll()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_cols = []\n",
    "def get_num_cols(_df):\n",
    "    global numeric_cols\n",
    "    numeric_cols = [x[0] for x in _df.dtypes if x[1] in ['int', 'double']]\n",
    "    return numeric_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_cols = []\n",
    "def get_string_cols(_df):\n",
    "    global string_cols\n",
    "    string_cols = [x[0] for x in _df.dtypes if x[1] == 'string' and x[0] not in ['HasDetections','MachineIdentifier']]\n",
    "    return string_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Plot:\n",
    "    \"\"\" \n",
    "    takes a col keyword for the column to plot, gets a pandas data frame and plots a sns catplot\n",
    "    of the top 40 col factors and their 'HasDetections' count\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, *args, **kwargs):\n",
    "        self.kwargs = {k:v for k,v in kwargs.items()}\n",
    "        self.col = kwargs.get('col')\n",
    "        self._df = kwargs.get('df')\n",
    "        self._plot(self._get_seaborn_pdf())\n",
    "        \n",
    "    def _get_seaborn_pdf(self):\n",
    "        pdf = (self._df.groupby(self.col, 'HasDetections')\n",
    "               .agg(F.count(self.col).alias(self.col+'Count'), F.count('HasDetections').alias('HasDetectionsCount'))\n",
    "               .sort(F.desc(self.col+'Count'))\n",
    "               .limit(40)\n",
    "               .selectExpr(\n",
    "                   self.col,self.col+'Count','HasDetections','HasDetectionsCount'\n",
    "               )).toPandas()\n",
    "        return pdf\n",
    "    \n",
    "    \n",
    "    def _plot(self, pdf):\n",
    "        g = sns.catplot(x=self.col, y='HasDetectionsCount', data=pdf, kind='bar', hue='HasDetections')\n",
    "        g.set_xticklabels(rotation=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cardinality(df):\n",
    "    col_distinct_counts = sorted([(_col, df.select(F.approx_count_distinct(_col, .1)).rdd.take(1)[0][0]) for _col in df.columns], key=lambda x: x[1], reverse=True)\n",
    "    return col_distinct_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pivot_plot(_df, col, count=10):\n",
    "    col_distinct_count = _df.select(col).distinct().count()\n",
    "    pdf = (_df.groupby(col, 'HasDetections')\n",
    "           .agg(F.count(col).alias(col+'Count'), F.count('HasDetections').alias('HasDetectionsCount'))\n",
    "           .sort(F.desc(col+'Count'))\n",
    "           .limit(count)\n",
    "           .selectExpr(\n",
    "               col,'HasDetections','HasDetectionsCount'\n",
    "           )).toPandas()\n",
    "    print(f\"Distinct values count: {col_distinct_count}\")\n",
    "    return pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_null_count(_df, _col):\n",
    "    # return count of null values in _col\n",
    "    df1 = _df.select(F.count(F.when(F.isnull(_col), _col)).alias('null_count'))\n",
    "    df2 = _df.groupby(_col).count().sort(F.desc('count'))\n",
    "    df1.show()\n",
    "    df2.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_io(df=None, operation='clean'):\n",
    "    if operation == 'orig':\n",
    "        print('reading in original train.csv')\n",
    "        df = (spark.read.format('csv')\n",
    "          .option('inferSchema', 'true')\n",
    "          .option('header', 'true')\n",
    "          .option('quote', '\\\"')\n",
    "          .option('escape', '\\\"')\n",
    "          .load('hdfs://namenode:9000/data/train.csv') # train|test.csv\n",
    "         )\n",
    "        \n",
    "        (df\n",
    "         .coalesce(1) \n",
    "         .write.format('parquet')\n",
    "         .mode('overwrite')\n",
    "         .save('hdfs://namenode:9000/data/train.parquet')\n",
    "        )\n",
    "        \n",
    "    elif operation == 'clean':\n",
    "        print('reading in train.parquet')\n",
    "        df = spark.read.parquet('hdfs://namenode:9000/data/train.parquet')\n",
    "        df.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "        df.count()\n",
    "        return df\n",
    "    \n",
    "    elif operation == 'train':\n",
    "        print('reading in df_fe.parquet')\n",
    "        df_train = spark.read.parquet('hdfs://namenode:9000/data/df_fe.parquet')\n",
    "        df_train.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "        df_train.count()\n",
    "        return df_train\n",
    "    \n",
    "    else:\n",
    "        print('reading in df_test.parquet')\n",
    "        df_test = spark.read.parquet('hdfs://namenode:9000/data/df_test.parquet')\n",
    "        df_test.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "        df_test.count()\n",
    "        return df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_io(operation='orig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = file_io(operation='clean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning/EDA\n",
    "* [x] TODO: Standardize *SmartScreen* values\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop columns with high occurence of null values > 70%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_df = df.select(\n",
    "    # get count of null values for each column\n",
    "    [(F.count(F.when(F.isnull(c), c))).alias(c) for c in df.columns]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create pandas dataframe: values are null counts of each column\n",
    "pdf = pd.DataFrame([_df.select(col).first() for col in _df.columns], _df.columns, columns=['count_null_values'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf = pdf.sort_values('count_null_values', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rows = df.count()\n",
    "pdf['null_ratio'] = pdf.count_null_values/num_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop_cols = ['PuaMode','Census_ProcessorClass','DefaultBrowsersIdentifier','Census_IsFlightingInternal','Census_InternalBatteryType']\n",
    "drop_cols = list(pdf[(pdf['null_ratio'] > .70)].index.values)\n",
    "drop_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(*drop_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impute remaining missing values to -1 for numeric and '-1' for string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature cardinality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "col_counts = [col for col in get_cardinality(df) if col[0] not in ['HasDetections','MachineIdentifier']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_num_cols(df)\n",
    "get_string_cols(df)\n",
    "df = df.fillna(-1, subset=numeric_cols)\n",
    "df = df.fillna('-1', subset=string_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify null values were changed to -1\n",
    "_pdf = df.select([F.count(F.when(F.isnull(col),col)).alias(col) for col in df.columns])\n",
    "_pdf = pd.DataFrame([_pdf.select(_col).first() for _col in _pdf.columns], _pdf.columns, columns=['null_count'])\n",
    "_pdf.sort_values('null_count',ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get skewness of columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df_skew = df.select(\n",
    "   [F.skewness(_col).alias(_col) for _col in df.columns]\n",
    ")\n",
    "pdf_skew = pd.DataFrame([df_skew.select(_col).first() for _col in df_skew.columns], df_skew.columns, columns=['skewness'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_skew.sort_values('skewness', ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize skewed features before dropping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = Plot(df=df, col='Census_PrimaryDiskTotalCapacity')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* This is a discriminating feature, but its cardinality is very high--perhaps use hashing trick/binning to reduce dimensionality\n",
    "* Hashing/binning reduces model interpretability\n",
    "* Will use frequency encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = Plot(df=df, col='UacLuaenable')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = Plot(df=df,col='SmartScreen')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* After reviewing the plot of this feature *SmartScreen*, decided not to drop it due to the potential discriminating effect at predicting P(Y|x)\n",
    "* TODO: write function to standardize values (e.g. off -> Off)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Plot(df=df,col='IsBeta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Plot(df=df,col='AutoSampleOptIn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop skewed columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# skew_drop_cols = ['IsBeta', 'AutoSampleOptIn', 'UacLuaenable']\n",
    "skew_drop_cols = pdf_skew[pdf_skew['skewness'] > 100]\n",
    "skew_drop_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skew_drop_cols = [col for col in list(skew_drop_cols.index.values) if col not in ['SmartScreen', 'Census_PrimaryDiskTotalCapacity']]\n",
    "skew_drop_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(*skew_drop_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze distribution of columns by HasDetections column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Plot(df=df,col='CountryIdentifier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Plot(df=df,col='Census_OSVersion')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Plot(df=df,col='RtpStateBitfield')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = Plot(df=df,col='EngineVersion')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = Plot(df=df,col='AppVersion')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = Plot(df=df,col='AvSigVersion')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get relative frequency of largest factor for each categorical feature\n",
    "* drop columns where one factor is greater than 90% of column density\n",
    "* decided not to drop certain columns because of their potential to predict P(Y|x); will attempt to filter features more using Chi-square statistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "def analyze_rel_freq():\n",
    "    cols = df.columns\n",
    "    rel_freqs = []\n",
    "    for col in cols:\n",
    "        rel_freqs.append((col, (df.groupby(col)\n",
    "         .count()\n",
    "         .withColumn('rel_freq', F.col('count')/df.count())\n",
    "         .orderBy(F.desc('count')).orderBy(F.desc('rel_freq'))\n",
    "        ).rdd.take(1)[0].rel_freq))\n",
    "    return rel_freqs\n",
    "rel_freqs = analyze_rel_freq()\n",
    "rel_freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_rel_freq_gt_90 = [x[0] for x in sorted(rel_freqs, key=lambda x: x[1], reverse=True) if x[1] > .9]\n",
    "drop_rel_freq_gt_90"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize skewed features before dropping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[Plot(df=df,col=_col) for _col in drop_rel_freq_gt_90]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_rel_freq_gt_90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_drop_cols = [col for col in drop_rel_freq_gt_90 if col not in ['IsProtected','Census_IsAlwaysOnAlwaysConnectedCapable','Census_FlightRing','Census_HasOpticalDiskDrive','OsVer','Census_HasOpticalDiskDrive','Census_IsVirtualDevice','Processor','RtpStateBitfield','AVProductsEnabled']]\n",
    "_drop_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_plot(df,'IsProtected')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_plot(df,'OsVer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_plot(df,'Census_HasOpticalDiskDrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_plot(df,'Census_IsVirtualDevice')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_plot(df,'Processor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_plot(df,'RtpStateBitfield')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_plot(df,'AVProductsEnabled')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(*_drop_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_plot(df,'SmartScreen')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_smart_screen(df):\n",
    "    df = (df.withColumn('SmartScreen', F.regexp_replace('SmartScreen', r'off', 'Off'))\n",
    "      .withColumn('SmartScreen', F.regexp_replace('SmartScreen', r'OFF', 'Off'))\n",
    "      .withColumn('SmartScreen', F.regexp_replace('SmartScreen', r'on', 'On'))\n",
    "      .withColumn('SmartScreen', F.regexp_replace('SmartScreen', r'&#x02;', '2'))\n",
    "      .withColumn('SmartScreen', F.regexp_replace('SmartScreen', r'&#x01;', '1'))\n",
    "      .withColumn('SmartScreen', F.regexp_replace('SmartScreen', r'&#x03;', '3'))\n",
    "      .withColumn('SmartScreen', F.regexp_replace('SmartScreen', r'requireadmin', 'RequireAdmin'))\n",
    "      .withColumn('SmartScreen', F.regexp_replace('SmartScreen', r'Promt', 'Prompt'))\n",
    "     )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = standardize_smart_screen(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_plot(df,'SmartScreen')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write cleaned/EDA DF to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (df\n",
    "#  .coalesce(1) \n",
    "#  .write.format('parquet')\n",
    "#  .mode('overwrite')\n",
    "#  .save('hdfs://namenode:9000/data/df_fe.parquet')\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "* [x] TODO: Onehot encode categorical data with low cardinality\n",
    "* [x] TODO: Frequency encode categorical data with high cardinality\n",
    "* [x] TODO: Create hashing transformer function\n",
    "* [ ] TODO: Consider ignoring -1 (null) values in FE pipeline\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read-in cleaned train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = file_io(operation='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = file_io(operation='test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reconcile train/test columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_drop_cols = [x for x in (set(df_test.columns) - set(df_train.columns)) if x != 'HasDetections']\n",
    "test_drop_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df_test.drop(*test_drop_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_drop_cols = [x for x in (set(df_train.columns) - set(df_test.columns)) if x != 'HasDetections']\n",
    "train_drop_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.drop(*train_drop_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df_train.columns), len(df_test.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_counts = get_cardinality(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_cardinality_cols = [tup[0] for tup in col_counts if tup[1] > 32]\n",
    "high_cardinality_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_cardinality_cols = [tup[0] for tup in col_counts if tup[1] < 32]\n",
    "low_cardinality_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_si = feature.StringIndexer(inputCols=high_cardinality_cols, outputCols=[col+'_si' for col in high_cardinality_cols])\n",
    "low_si = feature.StringIndexer(inputCols=low_cardinality_cols, outputCols=[col+'_si' for col in low_cardinality_cols])\n",
    "low_oh = feature.OneHotEncoder(inputCols=low_si.getOutputCols(), outputCols=[col+'_oh' for col in low_si.getOutputCols()])\n",
    "va = feature.VectorAssembler(inputCols=high_si.getOutputCols() + low_oh.getOutputCols(), outputCol='features')\n",
    "\n",
    "enc_pipeline_fit = Pipeline(stages=[\n",
    "    high_si,\n",
    "    low_si,\n",
    "    low_oh,\n",
    "    va\n",
    "]).fit(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_pipeline_trans = enc_pipeline_fit.transform(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_pipeline_trans.persist(StorageLevel.MEMORY_AND_DISK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_pipeline_trans.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ChiSquareTest feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chi test statistic algorithm seems to not handle features with cardinality > 10000\n",
    "cols = [x[0] for x in col_counts if x[1] < 10000 and x[0] not in ['MachineIdentifier','HasDetections']] \n",
    "# cols = [x[0] for x in fe_distinct_count if x[0] not in ['MachineIdentifier','HasDetections']] \n",
    "cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chi_test_df = spark.read.parquet('hdfs://namenode:9000/data/df_fe.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexer = feature.StringIndexer(inputCols=cols, outputCols=[col + \"_index\" for col in cols])\n",
    "va = feature.VectorAssembler(inputCols=indexer.getOutputCols(), outputCol='features')\n",
    "\n",
    "def chi_test_pipeline(df):\n",
    "    transformed_df = Pipeline(stages=[\n",
    "        indexer,\n",
    "        va\n",
    "    ]).fit(df).transform(df).select('features','HasDetections')\n",
    "    return transformed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi_test_df = chi_test_pipeline(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi_test_df.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "chi_test_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from pyspark.ml.stat import ChiSquareTest\n",
    "\n",
    "\n",
    "r = ChiSquareTest.test(chi_test_df, \"features\", \"HasDetections\").head()\n",
    "print(\"pValues: \" + str(r.pValues))\n",
    "print(\"degreesOfFreedom: \" + str(r.degreesOfFreedom))\n",
    "print(\"statistics: \" + str(r.statistics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "chi_pdf = pd.DataFrame(r.statistics.toArray(), index=cols, columns=['chi_stat']).sort_values('chi_stat',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(11, 9))\n",
    "g = (sns.barplot(x='chi_stat', y=chi_pdf.index,data=chi_pdf)\n",
    "#      .set_xticklabels(labels='chi_stat', rotation=90)     \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select features that have a relationship with target; cut off at EngineVersion as observed versus expected values diverge exponentially from there \n",
    "chi_stat_relationship_cols = chi_pdf.loc[chi_pdf['chi_stat'] < 144878, 'chi_stat'].index.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi_pdf.sort_values('chi_stat', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select features that have a relationship with the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi_stat_relationship_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi_keep_cols = list(set(df_train.columns).intersection(chi_stat_relationship_cols)) + ['MachineIdentifier','HasDetections']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi_keep_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(cols) - set(chi_keep_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.select(chi_keep_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df_test.select([col for col in chi_keep_cols if col != 'HasDetections'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df_train.columns), len(df_test.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hashed_df_train, hashed_df_validation = hashed_df.randomSplit([.9, .1], seed=0)\n",
    "hashed_df_train, hashed_df_validation = df_train.randomSplit([.9, .1], seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashed_df_train.persist(StorageLevel.MEMORY_ONLY)\n",
    "hashed_df_train.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashed_df_validation.persist(StorageLevel.MEMORY_ONLY)\n",
    "hashed_df_validation.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation Analaysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.stat import Correlation\n",
    "\n",
    "r1 = Correlation.corr(chi_test_df, \"features\").collect()[0][0]\n",
    "# print(\"Pearson correlation matrix:\\n\" + str(r1[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrmatrix = r1.toArray().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_df = spark.createDataFrame(corrmatrix, cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_corr_pdf = corr_df.toPandas()\n",
    "pdf_corr_pdf.index = corr_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From https://seaborn.pydata.org/examples/many_pairwise_correlations.html\n",
    "\n",
    "# Generate a mask for the upper triangle\n",
    "mask = np.triu(np.ones_like(pdf_corr_pdf, dtype=np.bool))\n",
    "\n",
    "f, ax = plt.subplots(figsize=(20,18))\n",
    "\n",
    "# Generate a custom diverging colormap\n",
    "cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "\n",
    "# Draw the heatmap with the mask and correct aspect ratio\n",
    "sns.heatmap(pdf_corr_pdf, mask=mask, cmap=cmap, vmax=.3, center=0,\n",
    "            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi_test_df.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [col for col in df_train.columns if x not in ['MachineIdentifier','HasDetections']] \n",
    "# cols = [col for col in df_train.columns if x not in ['MachineIdentifier','HasDetections'] + chi_stat_relationship_cols]\n",
    "\n",
    "def modeling_pipeline(df, test=False):\n",
    "    cols = [col for col in df.columns if col not in ['MachineIdentifier','HasDetections']]    \n",
    "    indexer = feature.StringIndexer(inputCols=cols, outputCols=[col + \"_index\" for col in cols])\n",
    "    one_hot = feature.OneHotEncoder(inputCols=indexer.getOutputCols(), outputCols=[col + \"_oh\" for col in cols])\n",
    "    va = feature.VectorAssembler(inputCols=indexer.getOutputCols(), outputCol='features')\n",
    "    \n",
    "    fitted_df = Pipeline(stages=[\n",
    "        indexer,\n",
    "        one_hot,\n",
    "        va\n",
    "    ]).fit(df)\n",
    "    \n",
    "    if test:\n",
    "        transformed_df = fitted_df.transform(df).select('MachineIdentifier','features')\n",
    "    else:\n",
    "        transformed_df = fitted_df.transform(df).select('MachineIdentifier','features','HasDetections')\n",
    "    return transformed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_df_fe = modeling_pipeline(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_df.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "training_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_df.persist(StorageLevel.MEMORY_ONLY)\n",
    "validation_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logestic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_df, validation_df = enc_pipeline_trans.randomSplit([0.8, 0.2], seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_df.persist(StorageLevel.MEMORY_AND_DISK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_pipeline_trans.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(featuresCol='features', labelCol='HasDetections')\n",
    "\n",
    "pipeline = Pipeline(stages=[\n",
    "    lr\n",
    "]).fit(training_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "hasher_cols = [col for col in chi_keep_cols if col not in ['MachineIdentifier','HasDetections']]\n",
    "\n",
    "hasher = feature.FeatureHasher(numFeatures=512,inputCols=hasher_cols,outputCol='hash_features', categoricalCols=hasher_cols)\n",
    "lr = LogisticRegression(featuresCol='hash_features', labelCol='label')\n",
    "\n",
    "pipeline = Pipeline(stages=[\n",
    "    hasher,\n",
    "    lr\n",
    "]).fit(hashed_df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.transform(validation_df).\\\n",
    "    select(F.expr('float(prediction = HasDetections)').alias('correct')).\\\n",
    "    select(F.avg('correct')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LR Model Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Loop over each parameter mapping in paramGrid and fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# hashed_df_train = hashed_df_train.withColumnRenamed('HasDetections', 'label')\n",
    "df_train = df_train.withColumnRenamed('HasDetections', 'label')\n",
    "# hasher_cols = [col for col in chi_keep_cols if col not in ['MachineIdentifier','HasDetections']]\n",
    "cols = [col for col in chi_keep_cols if col not in ['MachineIdentifier','HasDetections']]    \n",
    "\n",
    "\n",
    "# transformers\n",
    "# hasher = feature.FeatureHasher(inputCols=hasher_cols,outputCol='hash_features', categoricalCols=hasher_cols)\n",
    "indexer = feature.StringIndexer(inputCols=cols, outputCols=[col + \"_index\" for col in cols], handleInvalid='keep')\n",
    "one_hot = feature.OneHotEncoder(inputCols=indexer.getOutputCols(), outputCols=[col + \"_oh\" for col in cols])\n",
    "va = feature.VectorAssembler(inputCols=one_hot.getOutputCols(), outputCol='features')\n",
    "lr = LogisticRegression(featuresCol='features', labelCol='label')\n",
    "\n",
    "_evaluator = BinaryClassificationEvaluator(labelCol='label', metricName='areaUnderROC')\n",
    "\n",
    "paramGrid = (ParamGridBuilder() \n",
    "#                  .addGrid(hasher.numFeatures, [256,512])\n",
    "#                  .addGrid(lr.regParam, [0.1, 0.01]) \n",
    "                 .addGrid(lr.elasticNetParam, [0.2, 0.5, 0.8]) \n",
    "                 .build()\n",
    "            )\n",
    "\n",
    "pipeline = Pipeline(stages=[\n",
    "#     hasher,\n",
    "    indexer,\n",
    "    one_hot,\n",
    "    va,\n",
    "    lr\n",
    "])\n",
    "\n",
    "tvs = TrainValidationSplit(estimator=pipeline,\n",
    "                           estimatorParamMaps=paramGrid,\n",
    "                           evaluator=_evaluator,\n",
    "                           trainRatio=0.8)\n",
    "\n",
    "lr_model = tvs.fit(df_train)\n",
    "# lr_model = tvs.fit(hashed_df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "# evaluator = BinaryClassificationEvaluator(labelCol='HasDetections', metricName='areaUnderROC')\n",
    "# lr_auc_scores = [evaluator.evaluate(model.transform(hashed_df_validation)) for model in lr_models]\n",
    "lr_model.bestModel.transform(hashed_df_validation).\\\n",
    "    select(F.expr('float(prediction = HasDetections)').alias('correct')).\\\n",
    "    select(F.avg('correct')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df_test.withColumnRenamed('HasDetections', 'label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_predict = lr_model.bestModel.transform(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submit = df_test_predict.select('MachineIdentifier', 'prediction').withColumnRenamed('prediction','HasDetections')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submit.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df_submit\n",
    " .coalesce(1) \n",
    " .write.format('csv')\n",
    " .mode('overwrite')\n",
    " .save('hdfs://namenode:9000/data/lr_en_submit.csv')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = [x.name for x in list(lr_model.bestModel.stages[3].extractParamMap().keys())]\n",
    "values = list(lr_model.bestModel.stages[3].extractParamMap().values())\n",
    "list(zip(keys,values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LR Model with regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "hasher = feature.FeatureHasher(numFeatures=512,inputCols=hasher_cols,outputCol='hash_features', categoricalCols=hasher_cols)\n",
    "lr = LogisticRegression(featuresCol='hash_features', labelCol='label', regParam=0.0, elasticNetParam=0.2)\n",
    "reg_pipeline = Pipeline(stages=[\n",
    "    hasher,\n",
    "    lr\n",
    "]).fit(hashed_df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_pipeline.transform(hashed_df_validation).\\\n",
    "    select(F.expr('float(prediction = HasDetections)').alias('correct')).\\\n",
    "    select(F.avg('correct')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean/FE test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_fe_test_dataset(df):\n",
    "#     if 'Census_SystemVolumeTotalCapacity' in df.columns:\n",
    "#         df = df.withColumn('Census_SystemVolumeTotalCapacity', discretize_Census_SystemVolumeTotalCapacity(df.Census_SystemVolumeTotalCapacity))\n",
    "#     if 'SmartScreen' in df.columns:\n",
    "#         df = descretize_smart_screen(df)\n",
    "#     if 'Census_PrimaryDiskTotalCapacity' in df.columns:\n",
    "#         df = df.withColumn('Census_PrimaryDiskTotalCapacity', discretize_primary_disk_total_capacity(df.Census_PrimaryDiskTotalCapacity))\n",
    "#     if 'EngineVersion' in df.columns:\n",
    "#         df = df.withColumn('EngineVersion', F.regexp_extract('EngineVersion',r'\\d.\\d+.(\\d+.\\d+)',1))\n",
    "    if 'AppVersion' in df.columns:\n",
    "        df = df.withColumn('AppVersion', F.regexp_extract('AppVersion',r'\\d.\\d+.(\\d+.\\d+)',1))\n",
    "    if 'AvSigVersion' in df.columns:\n",
    "        df = df.withColumn('AvSigVersion', F.regexp_extract('AvSigVersion',r'\\d.\\d+.(\\d+).\\d+',1))\n",
    "    if 'Census_InternalBatteryNumberOfCharges' in df.columns:\n",
    "        df = df.drop('Census_InternalBatteryNumberOfCharges')\n",
    "    if 'Census_TotalPhysicalRAM' in df.columns:\n",
    "        df = (df.withColumn('Census_TotalPhysicalRAM', F.col('Census_TotalPhysicalRAM')/1024)\n",
    "              .withColumn('Census_TotalPhysicalRAM', F.round(F.col('Census_TotalPhysicalRAM'),1))\n",
    "             )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_test = spark.read.parquet('hdfs://namenode:9000/data/df_test.parquet')\n",
    "# df_test.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "# df_test.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_fe = clean_fe_test_dataset(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_fe = modeling_pipeline(df_test_fe, test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_fe.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "df_test_fe.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make predictions on test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_predict = pipeline.transform(df_test_fe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_predict.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submit = df_test_predict.select('MachineIdentifier', 'prediction').withColumnRenamed('prediction','HasDetections')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submit.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (df_submit\n",
    "#  .coalesce(1) \n",
    "#  .write.format('csv')\n",
    "#  .mode('overwrite')\n",
    "#  .save('hdfs://namenode:9000/data/df_submit.csv')\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest\n",
    "* OOM errors; believe these are occuring because of high cardinality of features\n",
    "* [x] Plan to use feature hasher to reduce cardinality\n",
    "* use this code block for RF model with chi square features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "hashed_df_train = hashed_df_train.withColumnRenamed('HasDetections', 'label')  # rf looks for label column\n",
    "hasher_cols = [col for col in chi_keep_cols if col not in ['MachineIdentifier','HasDetections']]\n",
    "\n",
    "hasher = feature.FeatureHasher(numFeatures=512,inputCols=hasher_cols,outputCol='hash_features', categoricalCols=hasher_cols)\n",
    "rf = RandomForestClassifier(featuresCol='hash_features', labelCol='label', maxDepth=15,\\\n",
    "                            numTrees=200, featureSubsetStrategy='sqrt', impurity='gini', seed=0)\n",
    "rf_pipeline = Pipeline(stages=[\n",
    "    hasher,\n",
    "    rf\n",
    "]).fit(hashed_df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_pipeline.transform(hashed_df_validation).\\\n",
    "    select(F.avg(F.expr('float(HasDetections = prediction)')).alias('accuracy')).\\\n",
    "    first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RF Model Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "hashed_df_train = hashed_df_train.withColumnRenamed('HasDetections', 'label')\n",
    "hasher_cols = [col for col in chi_keep_cols if col not in ['MachineIdentifier','HasDetections']]\n",
    "\n",
    "# transformers\n",
    "hasher = feature.FeatureHasher(inputCols=hasher_cols,outputCol='hash_features', categoricalCols=hasher_cols)\n",
    "rf = RandomForestClassifier(featuresCol='hash_features', labelCol='label', featureSubsetStrategy='sqrt', impurity='gini', seed=0)\n",
    "\n",
    "_evaluator = BinaryClassificationEvaluator(labelCol='label', metricName='areaUnderROC')\n",
    "\n",
    "paramGrid = (ParamGridBuilder() \n",
    "                .addGrid(hasher.numFeatures, [1024]) \n",
    "                .addGrid(rf.numTrees, [150]) \n",
    "                .addGrid(rf.maxDepth, [15]) \n",
    "                .build()\n",
    "            )\n",
    "\n",
    "rf_pipeline = Pipeline(stages=[\n",
    "    hasher,\n",
    "    rf\n",
    "])\n",
    "\n",
    "tvs = TrainValidationSplit(estimator=rf_pipeline,\n",
    "                           estimatorParamMaps=paramGrid,\n",
    "                           evaluator=_evaluator,\n",
    "                           trainRatio=0.8)\n",
    "\n",
    "rf_model = tvs.fit(hashed_df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model.bestModel.transform(hashed_df_validation).\\\n",
    "    select(F.expr('float(prediction = HasDetections)').alias('correct')).\\\n",
    "    select(F.avg('correct')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.mllib.tree import GradientBoostedTrees, GradientBoostedTreesModel\n",
    "\n",
    "hashed_df_train = hashed_df_train.withColumnRenamed('HasDetections', 'label')\n",
    "hasher_cols = [col for col in chi_keep_cols if col not in ['MachineIdentifier','HasDetections']]\n",
    "\n",
    "# transformers\n",
    "hasher_cols = [col for col in chi_keep_cols if col not in ['MachineIdentifier','HasDetections']]\n",
    "hasher = feature.FeatureHasher(inputCols=hasher_cols,outputCol='hash_features', categoricalCols=hasher_cols)\n",
    "gb = GradientBoostedTrees.trainClassifier(trainingData, categoricalFeaturesInfo={}, numIterations=3)\n",
    "\n",
    "_evaluator = BinaryClassificationEvaluator(labelCol='label', metricName='areaUnderROC')\n",
    "\n",
    "paramGrid = (ParamGridBuilder() \n",
    "                .addGrid(hasher.numFeatures, [256, 512]) \\\n",
    "                .addGrid(gb.numIterations, [100,150]) \\ \n",
    "                .build()\n",
    "            )\n",
    "\n",
    "pipeline = Pipeline(stages=[\n",
    "    hasher,\n",
    "    gb\n",
    "])\n",
    "\n",
    "tvs = TrainValidationSplit(estimator=pipeline,\n",
    "                           estimatorParamMaps=paramGrid,\n",
    "                           evaluator=_evaluator,\n",
    "                           trainRatio=0.8)\n",
    "\n",
    "rf_model = tvs.fit(hashed_df_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.withColumnRenamed('HasDetections', 'label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# grid search works, but is too computationaly expensive for local dev--need to deploy in a cloud environment\n",
    "\n",
    "hasher_cols = [col for col in chi_keep_cols if col not in ['MachineIdentifier','HasDetections']]\n",
    "hasher = feature.FeatureHasher(inputCols=hasher_cols,outputCol='hash_features', categoricalCols=hasher_cols)\n",
    "rf = RandomForestClassifier(featuresCol='hash_features', labelCol='label', featureSubsetStrategy='sqrt', impurity='gini', seed=0)\n",
    "\n",
    "rf_pipeline = Pipeline(stages=[\n",
    "    hasher,\n",
    "    rf\n",
    "])\n",
    "\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(hasher.numFeatures, [64, 128]) \\\n",
    "    .addGrid(rf.numTrees, [10, 50, 100]) \\\n",
    "    .addGrid(rf.maxDepth, [5, 10, 15]) \\\n",
    "    .build()\n",
    "\n",
    "crossval = CrossValidator(estimator=rf_pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=BinaryClassificationEvaluator(),\n",
    "                          numFolds=3)\n",
    "\n",
    "cvModel = crossval.fit(df_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* crossval object will gridsearch over parameters specified in paramGrid. 54 models (2 * 3 * 3 * 3 folds) will be generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashed_df_test = feature_hasher(df_test, test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashed_df_test.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "hashed_df_test.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_test_results = rf_pipeline.transform(hashed_df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_submit = rf_test_results.select('MachineIdentifier', 'prediction').withColumnRenamed('prediction','HasDetections')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_submit.persist(StorageLevel.MEMORY_ONLY)\n",
    "rf_submit.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (rf_submit\n",
    "#  .coalesce(1) \n",
    "#  .write.format('csv')\n",
    "#  .mode('overwrite')\n",
    "#  .save('hdfs://namenode:9000/data/rf_chi_features_submit.csv')\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./ms_kaggle.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
