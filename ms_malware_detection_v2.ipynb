{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary of Project\n",
    "* This notebook models data from https://www.kaggle.com/c/microsoft-malware-prediction/overview\n",
    "* Used feature engineering concepts from these kaggle kernels: https://www.kaggle.com/jiegeng94/everyone-do-this-at-the-beginning, https://www.kaggle.com/cdeotte/high-scoring-lgbm-malware-0-702-0-775#Why-did-this-model-come-in-1435th-place?\n",
    "* Random Forest model generalizes better than the Logistic Regression with regularization; belive this is because of how bagging minimizes variance by aggregating weak predictors (trees) and sampled feature subsets into a stronger predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "---\n",
    "* [x] reduce cardinality of features using frequency|binary encoding\n",
    "* [x] grid search LR model regParam and elasticNet parameters \n",
    "* [x] create random forest model\n",
    "* [x] grid search RF model\n",
    "* [x] create gradient-boosted tree models\n",
    "* [x] enable Adaptive Query Execution to handle data skew, shuffle, and join logic\n",
    "* [x] fix driver.maxResultSize error\n",
    "* [x] select features from chi square statistic analysis\n",
    "* [ ] handle correlated features\n",
    "* [ ] implement features from kaggle kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment setup\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.conf import SparkConf\n",
    "from pyspark.storagelevel import StorageLevel\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.ml import feature, evaluation, Pipeline\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = (SparkConf()\n",
    "            .setAppName('ms_malware_pred')\n",
    "            .setMaster('spark://spark-master:7077')\n",
    "       )\n",
    "conf.set(\"spark.executor.cores\", \"2\")\n",
    "conf.set(\"spark.executor.memory\", \"16g\")\n",
    "conf.set(\"spark.driver.maxResultSize\", \"1g\")\n",
    "conf.set(\"spark.default.parallelism\", \"6\")  # default number of cores in application\n",
    "conf.set(\"spark.driver.memory\", \"3g\") \n",
    "conf.set(\"spark.shuffle.file.buffer\", \"1m\")\n",
    "conf.set(\"spark.shuffle.unsafe.file.output.buffer\", \"1m\")\n",
    "conf.set(\"spark.sql.shuffle.partitions\", \"18\")  # 3 * cores available (6)\n",
    "conf.set(\"spark.sql.adaptive.enabled\", \"true\") # enable Adaptive Query Execution\n",
    "conf.set(\"spark.kryoserializer.buffer.max\", \"1g\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext.getConf().getAll()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_cols = []\n",
    "def get_num_cols(_df):\n",
    "    global numeric_cols\n",
    "    numeric_cols = [x[0] for x in _df.dtypes if x[1] in ['int', 'double']]\n",
    "    return numeric_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_cols = []\n",
    "def get_string_cols(_df):\n",
    "    global string_cols\n",
    "    string_cols = [x[0] for x in _df.dtypes if x[1] == 'string' and x[0] not in ['HasDetections','MachineIdentifier']]\n",
    "    return string_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Plot:\n",
    "    \"\"\" \n",
    "    takes a col keyword for the column to plot, gets a pandas data frame and plots a sns catplot\n",
    "    of the top 40 col factors and their 'HasDetections' count\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, *args, **kwargs):\n",
    "        self.kwargs = {k:v for k,v in kwargs.items()}\n",
    "        self.col = kwargs.get('col')\n",
    "        self._df = kwargs.get('df')\n",
    "        self._plot(self._get_seaborn_pdf())\n",
    "        \n",
    "    def _get_seaborn_pdf(self):\n",
    "        pdf = (self._df.groupby(self.col, 'HasDetections')\n",
    "               .agg(F.count(self.col).alias(self.col+'Count'), F.count('HasDetections').alias('HasDetectionsCount'))\n",
    "               .sort(F.desc(self.col+'Count'))\n",
    "               .limit(40)\n",
    "               .selectExpr(\n",
    "                   self.col,self.col+'Count','HasDetections','HasDetectionsCount'\n",
    "               )).toPandas()\n",
    "        return pdf\n",
    "    \n",
    "    \n",
    "    def _plot(self, pdf):\n",
    "        g = sns.catplot(x=self.col, y='HasDetectionsCount', data=pdf, kind='bar', hue='HasDetections')\n",
    "        g.set_xticklabels(rotation=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Plot2(Plot):\n",
    "    \"\"\" line plot that shows top categories' detection rate \"\"\"\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        self.kwargs = {k:v for k,v in kwargs.items()}\n",
    "        super().__init__(df=kwargs.get('df'), col=kwargs.get('col'))\n",
    "        \n",
    "    def _plot(self, pdf):\n",
    "        g = sns.catplot(x=self.col, y='HasDetectionsCount', data=pdf, kind='bar', hue='HasDetections')\n",
    "        g.set_xticklabels(rotation=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cardinality(df):\n",
    "    col_distinct_counts = sorted([(_col, df.select(F.approx_count_distinct(_col, .1)).rdd.take(1)[0][0]) for _col in df.columns], key=lambda x: x[1], reverse=True)\n",
    "    return col_distinct_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pivot_plot(_df, col, count=10):\n",
    "    col_distinct_count = _df.select(col).distinct().count()\n",
    "    pdf = (_df.groupby(col, 'HasDetections')\n",
    "           .agg(F.count(col).alias(col+'Count'), F.count('HasDetections').alias('HasDetectionsCount'))\n",
    "           .sort(F.desc(col+'Count'))\n",
    "           .limit(count)\n",
    "           .selectExpr(\n",
    "               col,'HasDetections','HasDetectionsCount'\n",
    "           )).toPandas()\n",
    "    print(f\"Distinct values count: {col_distinct_count}\")\n",
    "    return pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_null_count(_df, _col):\n",
    "    # return count of null values in _col\n",
    "    df1 = _df.select(F.count(F.when(F.isnull(_col), _col)).alias('null_count'))\n",
    "    df2 = _df.groupby(_col).count().sort(F.desc('count'))\n",
    "    df1.show()\n",
    "    df2.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_io(df=None, operation='clean'):\n",
    "    if operation == 'orig':\n",
    "        print('reading in original train.csv')\n",
    "        df = (spark.read.format('csv')\n",
    "          .option('inferSchema', 'true')\n",
    "          .option('header', 'true')\n",
    "          .option('quote', '\\\"')\n",
    "          .option('escape', '\\\"')\n",
    "          .load('hdfs://namenode:9000/data/train.csv') # train|test.csv\n",
    "         )\n",
    "        \n",
    "        (df\n",
    "         .coalesce(1) \n",
    "         .write.format('parquet')\n",
    "         .mode('overwrite')\n",
    "         .save('hdfs://namenode:9000/data/train.parquet')\n",
    "        )\n",
    "        \n",
    "    elif operation == 'clean':\n",
    "        print('reading in train.parquet')\n",
    "        df = spark.read.parquet('hdfs://namenode:9000/data/train.parquet')\n",
    "        df.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "        df.count()\n",
    "        return df\n",
    "    \n",
    "    elif operation == 'train':\n",
    "        print('reading in df_fe.parquet')\n",
    "        df_train = spark.read.parquet('hdfs://namenode:9000/data/df_fe.parquet')\n",
    "        df_train.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "        df_train.count()\n",
    "        return df_train\n",
    "    \n",
    "    else:\n",
    "        print('reading in df_test.parquet')\n",
    "        df_test = spark.read.parquet('hdfs://namenode:9000/data/df_test.parquet')\n",
    "        df_test.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "        df_test.count()\n",
    "        return df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_io(operation='orig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = file_io(operation='clean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning/EDA\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop columns with high occurence of null values > 70%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_df = df.select(\n",
    "    # get count of null values for each column\n",
    "    [(F.count(F.when(F.isnull(c), c))).alias(c) for c in df.columns]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create pandas dataframe: values are null counts of each column\n",
    "pdf = pd.DataFrame([_df.select(col).first() for col in _df.columns], _df.columns, columns=['count_null_values'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf = pdf.sort_values('count_null_values', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rows = df.count()\n",
    "pdf['null_ratio'] = pdf.count_null_values/num_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop_cols = ['PuaMode','Census_ProcessorClass','DefaultBrowsersIdentifier','Census_IsFlightingInternal','Census_InternalBatteryType']\n",
    "drop_cols = list(pdf[(pdf['null_ratio'] > .70)].index.values)\n",
    "drop_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(*drop_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impute remaining missing values to -1 for numeric and '-1' for string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature cardinality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "col_counts = [col for col in get_cardinality(df) if col[0] not in ['HasDetections','MachineIdentifier']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_num_cols(df)\n",
    "get_string_cols(df)\n",
    "df = df.fillna(-1, subset=numeric_cols)\n",
    "df = df.fillna('-1', subset=string_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify null values were changed to -1\n",
    "_pdf = df.select([F.count(F.when(F.isnull(col),col)).alias(col) for col in df.columns])\n",
    "_pdf = pd.DataFrame([_pdf.select(_col).first() for _col in _pdf.columns], _pdf.columns, columns=['null_count'])\n",
    "_pdf.sort_values('null_count',ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get skewness of columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df_skew = df.select(\n",
    "   [F.skewness(_col).alias(_col) for _col in df.columns]\n",
    ")\n",
    "pdf_skew = pd.DataFrame([df_skew.select(_col).first() for _col in df_skew.columns], df_skew.columns, columns=['skewness'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_skew.sort_values('skewness', ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize skewed features before dropping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = Plot(df=df, col='Census_PrimaryDiskTotalCapacity')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* This is a discriminating feature, but its cardinality is very high--perhaps use hashing trick/binning to reduce dimensionality\n",
    "* Hashing/binning reduces model interpretability\n",
    "* Will use frequency encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = Plot(df=df, col='UacLuaenable')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = Plot(df=df,col='SmartScreen')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* After reviewing the plot of this feature *SmartScreen*, decided not to drop it due to the potential discriminating effect at predicting P(Y|x)\n",
    "* TODO: write function to standardize values (e.g. off -> Off)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Plot(df=df,col='IsBeta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Plot(df=df,col='AutoSampleOptIn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop skewed columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# skew_drop_cols = ['IsBeta', 'AutoSampleOptIn', 'UacLuaenable']\n",
    "skew_drop_cols = pdf_skew[pdf_skew['skewness'] > 100]\n",
    "skew_drop_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skew_drop_cols = [col for col in list(skew_drop_cols.index.values) if col not in ['SmartScreen', 'Census_PrimaryDiskTotalCapacity']]\n",
    "skew_drop_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(*skew_drop_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze distribution of columns by HasDetections column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Plot(df=df,col='CountryIdentifier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Plot(df=df,col='Census_OSVersion')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Plot(df=df,col='RtpStateBitfield')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = Plot(df=df,col='EngineVersion')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = Plot(df=df,col='AppVersion')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = Plot(df=df,col='AvSigVersion')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get relative frequency of largest factor for each categorical feature\n",
    "* drop columns where one factor is greater than 90% of column density\n",
    "* decided not to drop certain columns because of their potential to predict P(Y|x); will attempt to filter features more using Chi-square statistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "def analyze_rel_freq():\n",
    "    cols = df.columns\n",
    "    rel_freqs = []\n",
    "    for col in cols:\n",
    "        rel_freqs.append((col, (df.groupby(col)\n",
    "         .count()\n",
    "         .withColumn('rel_freq', F.col('count')/df.count())\n",
    "         .orderBy(F.desc('count')).orderBy(F.desc('rel_freq'))\n",
    "        ).rdd.take(1)[0].rel_freq))\n",
    "    return rel_freqs\n",
    "rel_freqs = analyze_rel_freq()\n",
    "rel_freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_rel_freq_gt_90 = [x[0] for x in sorted(rel_freqs, key=lambda x: x[1], reverse=True) if x[1] > .9]\n",
    "drop_rel_freq_gt_90"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize skewed features before dropping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[Plot(df=df,col=_col) for _col in drop_rel_freq_gt_90]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_rel_freq_gt_90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_plot(df,'IsProtected')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_plot(df,'OsVer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_plot(df,'Census_HasOpticalDiskDrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_plot(df,'Census_IsVirtualDevice')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_plot(df,'Processor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_plot(df,'RtpStateBitfield')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_plot(df,'AVProductsEnabled')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_plot(df,'SmartScreen')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_drop_cols = [col for col in drop_rel_freq_gt_90 if col not in ['IsProtected','Census_IsAlwaysOnAlwaysConnectedCapable','Census_HasOpticalDiskDrive','OsVer','Census_HasOpticalDiskDrive','Census_IsVirtualDevice','Processor','RtpStateBitfield','AVProductsEnabled']]\n",
    "_drop_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(*_drop_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_smart_screen(df):\n",
    "    df = (df.withColumn('SmartScreen', F.regexp_replace('SmartScreen', r'off', 'Off'))\n",
    "      .withColumn('SmartScreen', F.regexp_replace('SmartScreen', r'OFF', 'Off'))\n",
    "      .withColumn('SmartScreen', F.regexp_replace('SmartScreen', r'on', 'On'))\n",
    "      .withColumn('SmartScreen', F.regexp_replace('SmartScreen', r'&#x02;', '2'))\n",
    "      .withColumn('SmartScreen', F.regexp_replace('SmartScreen', r'&#x01;', '1'))\n",
    "      .withColumn('SmartScreen', F.regexp_replace('SmartScreen', r'&#x03;', '3'))\n",
    "      .withColumn('SmartScreen', F.regexp_replace('SmartScreen', r'requireadmin', 'RequireAdmin'))\n",
    "      .withColumn('SmartScreen', F.regexp_replace('SmartScreen', r'Promt', 'Prompt'))\n",
    "     )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = standardize_smart_screen(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_plot(df,'SmartScreen')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write cleaned/EDA DF to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df\n",
    " .coalesce(1) \n",
    " .write.format('parquet')\n",
    " .mode('overwrite')\n",
    " .save('hdfs://namenode:9000/data/df_fe.parquet')\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read-in cleaned train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = file_io(operation='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = file_io(operation='test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reconcile train/test columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_drop_cols = [x for x in (set(df_test.columns) - set(df_train.columns)) if x != 'HasDetections'] + ['Census_InternalPrimaryDisplayResolutionHorizontal']\n",
    "test_drop_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df_test.drop(*test_drop_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_drop_cols = [x for x in (set(df_train.columns) - set(df_test.columns)) if x != 'HasDetections'] \n",
    "train_drop_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.drop(*train_drop_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df_train.columns), len(df_test.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_counts = get_cardinality(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.select('Census_PrimaryDiskTotalCapacity').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_cardinality_cols = [tup[0] for tup in col_counts if tup[1] > 32 and tup[0] not in ['MachineIdentifier']]\n",
    "high_cardinality_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_cardinality_cols = [tup[0] for tup in col_counts if tup[1] <= 32 and tup[0] not in ['HasDetections']]\n",
    "low_cardinality_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ChiSquareTest feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chi test statistic algorithm seems to not handle features with cardinality > 10000\n",
    "cols = [x[0] for x in col_counts if x[1] < 10000 and x[0] not in ['MachineIdentifier']] \n",
    "# cols = [x[0] for x in fe_distinct_count if x[0] not in ['MachineIdentifier','HasDetections']] \n",
    "cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import FeatureHasher\n",
    "\n",
    "dataset = spark.createDataFrame([\n",
    "    (2.2, True, 1, \"foo\"),\n",
    "    (2.2, True, 1, \"foo\"),\n",
    "    (3.3, False, 2, \"bar\"),\n",
    "    (4.4, False, 3, \"baz\"),\n",
    "    (5.5, False, 4, \"foo\"),\n",
    "    (5.5, False, 5, \"foo\"),\n",
    "    (5.5, False, 6, \"foo\")\n",
    "], [\"real\", \"bool\", \"stringNum\", \"string\"])\n",
    "\n",
    "hasher = FeatureHasher(inputCols=[\"real\", \"bool\", \"stringNum\", \"string\"],\n",
    "                       outputCol=\"features\", categoricalCols=[\"real\", \"stringNum\", \"string\"])\n",
    "\n",
    "featurized = hasher.transform(dataset)\n",
    "featurized.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import FeatureHasher\n",
    "\n",
    "dataset = spark.createDataFrame([\n",
    "    (2.2, True, 1, \"foo\"),\n",
    "    (2.2, True, 1, \"foo\"),\n",
    "    (3.3, False, 2, \"bar\"),\n",
    "    (4.4, False, 3, \"baz\"),\n",
    "    (5.5, False, 4, \"foo\"),\n",
    "    (5.5, False, 5, \"foo\"),\n",
    "    (5.5, False, 6, \"foo\")\n",
    "], [\"real\", \"bool\", \"stringNum\", \"string\"])\n",
    "\n",
    "hasher = FeatureHasher(inputCols=[\"real\", \"bool\", \"stringNum\", \"string\"],\n",
    "                       outputCol=\"features\")\n",
    "\n",
    "featurized = hasher.transform(dataset)\n",
    "featurized.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurized.select('features').rdd.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hasher = feature.FeatureHasher(inputCols=[col for col in df_train.columns if col != 'MachineIdentifier'],outputCol='hash_features')\n",
    "# indexer = feature.StringIndexer(inputCols=cols, outputCols=[col + \"_index\" for col in cols])\n",
    "# va = feature.VectorAssembler(inputCols=hasher.getOutputCol(), outputCol='features')\n",
    "\n",
    "def chi_test_pipeline(df):\n",
    "    fit_df = Pipeline(stages=[\n",
    "#         indexer,\n",
    "        hasher,\n",
    "#         va\n",
    "    ]).fit(df)\n",
    "    return fit_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi_test_fit = chi_test_pipeline(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi_test_transform = chi_test_fit.transform(df_train).select('hash_features','HasDetections')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi_test_transform.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "chi_test_transform.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from pyspark.ml.stat import ChiSquareTest\n",
    "\n",
    "\n",
    "r = ChiSquareTest.test(chi_test_transform, \"hash_features\", \"HasDetections\").head()\n",
    "print(\"pValues: \" + str(r.pValues))\n",
    "print(\"degreesOfFreedom: \" + str(r.degreesOfFreedom))\n",
    "print(\"statistics: \" + str(r.statistics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "chi_pdf = pd.DataFrame(r.statistics.toArray(), index=cols, columns=['chi_stat']).sort_values('chi_stat',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(11, 9))\n",
    "g = (sns.barplot(x='chi_stat', y=chi_pdf.index,data=chi_pdf)\n",
    "#      .set_xticklabels(labels='chi_stat', rotation=90)     \n",
    "     \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi_pdf.sort_values('chi_stat', ascending=True).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select features where the observed frequency values drastically diverge from the expected values -- indicating that this feature is predictive of target\n",
    "# TODO: I arbitrarily picked the cut-off statistic value; research a smarter way to do this\n",
    "chi_stat_relationship_cols = chi_pdf.loc[chi_pdf['chi_stat'] > 7715, 'chi_stat'].index.to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi_stat_relationship_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intersection = list(set(df_train.columns).intersection(set(chi_stat_relationship_cols)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi_keep_cols = intersection + list(set(cols) - set(intersection)) + ['HasDetections']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.select(chi_keep_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df_test.select([col for col in chi_keep_cols if col != 'HasDetections'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df_train.columns), len(df_test.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation Analaysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.stat import Correlation\n",
    "\n",
    "r1 = Correlation.corr(chi_test_transform, \"features\").collect()[0][0]\n",
    "# print(\"Pearson correlation matrix:\\n\" + str(r1[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrmatrix = r1.toArray().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_df = spark.createDataFrame(corrmatrix, cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_corr_pdf = corr_df.toPandas()\n",
    "pdf_corr_pdf.index = corr_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From https://seaborn.pydata.org/examples/many_pairwise_correlations.html\n",
    "\n",
    "# Generate a mask for the upper triangle\n",
    "# mask = np.triu(np.ones_like(pdf_corr_pdf, dtype=np.bool))\n",
    "\n",
    "f, ax = plt.subplots(figsize=(20,18))\n",
    "\n",
    "# Generate a custom diverging colormap\n",
    "cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "\n",
    "# Draw the heatmap with the mask and correct aspect ratio\n",
    "sns.heatmap(pdf_corr_pdf, cmap=cmap, center=0,\n",
    "            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi_test_transform.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection from Kaggle Kernel\n",
    "* https://www.kaggle.com/cdeotte/high-scoring-lgbm-malware-0-702-0-775"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_df, validation_df = df_train.randomSplit([0.8, 0.2], seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_df.persist(StorageLevel.MEMORY_AND_DISK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_df.persist(StorageLevel.MEMORY_AND_DISK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature encoding pipeline\n",
    "* [x] TODO: Onehot encode categorical data with low cardinality\n",
    "* [x] TODO: Frequency encode categorical data with high cardinality\n",
    "* [ ] TODO: Create hashing transformer function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_si = (feature.StringIndexer(inputCols=high_cardinality_cols, outputCols=[col+'_si' for col in high_cardinality_cols])).setHandleInvalid('keep')\n",
    "low_si = (feature.StringIndexer(inputCols=low_cardinality_cols, outputCols=[col+'_si' for col in low_cardinality_cols])).setHandleInvalid('keep')\n",
    "low_oh = (feature.OneHotEncoder(inputCols=low_si.getOutputCols(), outputCols=[col+'_oh' for col in low_si.getOutputCols()]))\n",
    "va = feature.VectorAssembler(inputCols=high_si.getOutputCols() + low_oh.getOutputCols(), outputCol='features')\n",
    "\n",
    "lr_enc_pipeline = Pipeline(stages=[\n",
    "    high_si,\n",
    "    low_si,\n",
    "    low_oh,\n",
    "    va\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logestic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(featuresCol='features', labelCol='HasDetections')\n",
    "\n",
    "pipeline = Pipeline(stages=[\n",
    "    lr\n",
    "]).fit(training_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.transform(validation_df).\\\n",
    "    select(F.expr('float(prediction = HasDetections)').alias('correct')).\\\n",
    "    select(F.avg('correct')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LR Model Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Loop over each parameter mapping in paramGrid and fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# training_df = training_df.withColumnRenamed('HasDetections', 'label')\n",
    "# cols = [col for col in chi_keep_cols if col not in ['MachineIdentifier','HasDetections']]    \n",
    "\n",
    "\n",
    "lr = LogisticRegression(featuresCol='features', labelCol='HasDetections')\n",
    "_evaluator = BinaryClassificationEvaluator(labelCol='HasDetections', metricName='areaUnderROC')\n",
    "\n",
    "paramGrid = (ParamGridBuilder() \n",
    "                 .addGrid(lr.elasticNetParam, [0.2, 0.5, 0.8]) \n",
    "                 .build()\n",
    "            )\n",
    "\n",
    "pipeline = Pipeline(stages=[\n",
    "    lr_enc_pipeline, # feature encoding pipeline\n",
    "    lr\n",
    "])\n",
    "\n",
    "tvs = TrainValidationSplit(estimator=pipeline,\n",
    "                           estimatorParamMaps=paramGrid,\n",
    "                           evaluator=_evaluator,\n",
    "                           trainRatio=0.8)\n",
    "\n",
    "lr_model = tvs.fit(df_train)\n",
    "# lr_model = tvs.fit(hashed_df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model.bestModel.stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "# evaluator = BinaryClassificationEvaluator(labelCol='HasDetections', metricName='areaUnderROC')\n",
    "# lr_auc_scores = [evaluator.evaluate(model.transform(hashed_df_validation)) for model in lr_models]\n",
    "lr_model.bestModel.transform(validation_df).\\\n",
    "    select(F.expr('float(prediction = HasDetections)').alias('correct')).\\\n",
    "    select(F.avg('correct')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = [x.name for x in list(lr_model.bestModel.stages[1].extractParamMap().keys())]\n",
    "values = list(lr_model.bestModel.stages[1].extractParamMap().values())\n",
    "list(zip(keys,values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make predictions on test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_predict = lr_model.bestModel.transform(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submit = df_test_predict.select('MachineIdentifier', 'prediction').withColumnRenamed('prediction','HasDetections')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submit.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df_submit\n",
    " .coalesce(1) \n",
    " .write.format('csv')\n",
    " .mode('overwrite')\n",
    " .save('hdfs://namenode:9000/data/lr_en_submit.csv')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest\n",
    "* use this code block for RF model with chi square features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_high_cardinality_cols = list(set(high_cardinality_cols).intersection(set(chi_keep_cols)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_high_cardinality_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_pipeline.stages[0].stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hasher = feature.FeatureHasher(numFeatures=32,inputCols=_high_cardinality_cols,outputCol='hash_features', categoricalCols=_high_cardinality_cols)\n",
    "low_si = (feature.StringIndexer(inputCols=low_cardinality_cols, outputCols=[col+'_si' for col in low_cardinality_cols])).setHandleInvalid('keep')\n",
    "low_oh = (feature.OneHotEncoder(inputCols=low_si.getOutputCols(), outputCols=[col+'_oh' for col in low_si.getOutputCols()]))\n",
    "va = feature.VectorAssembler(inputCols=[hasher.getOutputCol()] + low_oh.getOutputCols(), outputCol='features')\n",
    "\n",
    "rf_enc_pipeline = Pipeline(stages=[\n",
    "    hasher,\n",
    "    low_si,\n",
    "    low_oh,\n",
    "    va\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "rf = RandomForestClassifier(featuresCol='features', labelCol='HasDetections', maxDepth=10,\\\n",
    "                            numTrees=10, featureSubsetStrategy='sqrt', impurity='gini', seed=0)\n",
    "\n",
    "rf_pipeline = Pipeline(stages=[\n",
    "    rf_enc_pipeline,\n",
    "    rf\n",
    "]).fit(training_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_pipeline.transform(validation_df).\\\n",
    "    select(F.avg(F.expr('float(HasDetections = prediction)')).alias('accuracy')).\\\n",
    "    first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RF Model Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(featuresCol='features', labelCol='HasDetections', featureSubsetStrategy='sqrt', impurity='gini', seed=0)\n",
    "_evaluator = BinaryClassificationEvaluator(labelCol='HasDetections', metricName='areaUnderROC')\n",
    "\n",
    "\n",
    "rf_pipeline_tuned = Pipeline(stages=[\n",
    "    rf_pipeline,\n",
    "    rf\n",
    "])\n",
    "\n",
    "paramGrid = (ParamGridBuilder() \n",
    "                .addGrid(rf_pipeline.stages[0].stages[0].numFeatures, [32,64]) \n",
    "                .addGrid(rf.numTrees, [50,100]) \n",
    "                .addGrid(rf.maxDepth, [5,10]) \n",
    "                .build()\n",
    "            )\n",
    "\n",
    "tvs = TrainValidationSplit(estimator=rf_pipeline,\n",
    "                           estimatorParamMaps=paramGrid,\n",
    "                           evaluator=_evaluator,\n",
    "                           trainRatio=0.8)\n",
    "\n",
    "rf_model = tvs.fit(training_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model.bestModel.transform(validation_df).\\\n",
    "    select(F.expr('float(prediction = HasDetections)').alias('correct')).\\\n",
    "    select(F.avg('correct')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model.bestModel.stages[0].stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = [x.name for x in list(rf_model.bestModel.stages[1].extractParamMap().keys())]\n",
    "values = list(rf_model.bestModel.stages[1].extractParamMap().values())\n",
    "list(zip(keys,values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_submit = rf_model.transform(df_test).select('MachineIdentifier', 'prediction').withColumnRenamed('prediction','HasDetections')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(rf_submit\n",
    " .coalesce(1) \n",
    " .write.csv(header=True, path='hdfs://namenode:9000/data/rf_submit.csv', mode=\"overwrite\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hasher = feature.FeatureHasher(numFeatures=32,inputCols=_high_cardinality_cols,outputCol='hash_features', categoricalCols=_high_cardinality_cols)\n",
    "low_si = (feature.StringIndexer(inputCols=low_cardinality_cols, outputCols=[col+'_si' for col in low_cardinality_cols])).setHandleInvalid('keep')\n",
    "low_oh = (feature.OneHotEncoder(inputCols=low_si.getOutputCols(), outputCols=[col+'_oh' for col in low_si.getOutputCols()]))\n",
    "va = feature.VectorAssembler(inputCols=[hasher.getOutputCol()] + low_oh.getOutputCols(), outputCol='features')\n",
    "\n",
    "rf_enc_pipeline = Pipeline(stages=[\n",
    "    hasher,\n",
    "    low_si,\n",
    "    low_oh,\n",
    "    va\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "\n",
    "gb = GBTClassifier(labelCol='HasDetections', featuresCol='features')\n",
    "_evaluator = BinaryClassificationEvaluator(labelCol='HasDetections', metricName='areaUnderROC')\n",
    "\n",
    "\n",
    "gb_pipeline = Pipeline(stages=[\n",
    "    rf_enc_pipeline, # reuse encoding pipeline from rf model\n",
    "    gb\n",
    "])\n",
    "\n",
    "paramGrid = (ParamGridBuilder() \n",
    "                .addGrid(hasher.numFeatures, [32,64])\n",
    "                .addGrid(gb.maxIter, [50,100])\n",
    "                .addGrid(gb.maxDepth, [5,10])\n",
    "                .build()\n",
    "            )\n",
    "\n",
    "tvs = TrainValidationSplit(estimator=gb_pipeline,\n",
    "                           estimatorParamMaps=paramGrid,\n",
    "                           evaluator=_evaluator,\n",
    "                           trainRatio=0.8)\n",
    "\n",
    "gb_model = tvs.fit(training_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_model.bestModel.transform(validation_df).\\\n",
    "    select(F.expr('float(prediction = HasDetections)').alias('correct')).\\\n",
    "    select(F.avg('correct')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_model.validationMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = [x.name for x in list(gb_model.bestModel.stages[1].extractParamMap().keys())]\n",
    "values = list(gb_model.bestModel.stages[1].extractParamMap().values())\n",
    "list(zip(keys,values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_submit = gb_model.bestModel.transform(df_test).select('MachineIdentifier', 'prediction').withColumnRenamed('prediction','HasDetections')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(gb_submit\n",
    " .coalesce(1) \n",
    " .write.csv(header=True, path='hdfs://namenode:9000/data/gb_submit.csv', mode=\"overwrite\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.withColumnRenamed('HasDetections', 'label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# grid search works, but is too computationaly expensive for local dev--need to deploy in a cloud environment\n",
    "\n",
    "hasher_cols = [col for col in chi_keep_cols if col not in ['MachineIdentifier','HasDetections']]\n",
    "hasher = feature.FeatureHasher(inputCols=hasher_cols,outputCol='hash_features', categoricalCols=hasher_cols)\n",
    "rf = RandomForestClassifier(featuresCol='hash_features', labelCol='label', featureSubsetStrategy='sqrt', impurity='gini', seed=0)\n",
    "\n",
    "rf_pipeline = Pipeline(stages=[\n",
    "    hasher,\n",
    "    rf\n",
    "])\n",
    "\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(hasher.numFeatures, [64, 128]) \\\n",
    "    .addGrid(rf.numTrees, [10, 50, 100]) \\\n",
    "    .addGrid(rf.maxDepth, [5, 10, 15]) \\\n",
    "    .build()\n",
    "\n",
    "crossval = CrossValidator(estimator=rf_pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=BinaryClassificationEvaluator(),\n",
    "                          numFolds=3)\n",
    "\n",
    "cvModel = crossval.fit(df_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* crossval object will gridsearch over parameters specified in paramGrid. 54 models (2 * 3 * 3 * 3 folds) will be generated"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc-showmarkdowntxt": true,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
