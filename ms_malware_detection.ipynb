{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.conf import SparkConf\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.ml import feature, evaluation, Pipeline\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import re\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = (SparkConf()\n",
    "            .setAppName('ms_malware_pred')\n",
    "            .setMaster('spark://spark-master:7077')\n",
    "       )\n",
    "conf.set(\"spark.executor.cores\", \"2\")\n",
    "conf.set(\"spark.executor.memory\", \"8g\")\n",
    "conf.set(\"spark.driver.maxResultSize\", \"0\")\n",
    "conf.set(\"spark.default.parallelism\", \"6\")  # default number of cores in application\n",
    "conf.set(\"spark.driver.memory\", \"6g\") \n",
    "conf.set(\"spark.ui.showConsoleProgress\", \"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext.getConf().getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (spark.read.format('csv')\n",
    "      .option('inferSchema', 'true')\n",
    "      .option('header', 'true')\n",
    "      .option('quote', '\\\"')\n",
    "      .option('escape', '\\\"')\n",
    "      .load('hdfs://namenode:9000/data/train.csv')\n",
    "     )\n",
    "df.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# added 'MachineIdentifier' in numeric list to enable common join \n",
    "numeric_cols = []\n",
    "def get_num_cols(_df):\n",
    "    global numeric_cols\n",
    "    numeric_cols = [x[0] for x in _df.dtypes if x[1] in ['int', 'double']]\n",
    "    return numeric_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_cols = []\n",
    "def get_string_cols(_df):\n",
    "    global string_cols\n",
    "    string_cols = [x[0] for x in _df.dtypes if x[1] == 'string']\n",
    "    return string_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_null_count(_df, _col):\n",
    "    # return count of null values in _col\n",
    "    df1 = _df.select(F.count(F.when(F.isnull(_col), _col)).alias('null_count'))\n",
    "    df2 = _df.groupby(_col).count().sort(F.desc('count'))\n",
    "    df1.show()\n",
    "    df2.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summ_stats(_col):\n",
    "    # return summary stats (e.g. min, max, mean, quantile for _col)\n",
    "    # this function isn't needed DF.summary() returns the same information faster\n",
    "    df.describe(_col).show()\n",
    "    print(df.approxQuantile(_col, [.25,.5,.75], 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pivot_plot(col, count=20):\n",
    "    col_distinct_count = df.select(col).distinct().count()\n",
    "    pdf = (df.groupby(col, 'HasDetections')\n",
    "           .agg(F.count(col).alias(col+'Count'), F.count('HasDetections').alias('HasDetectionsCount'))\n",
    "           .sort(F.desc(col+'Count'))\n",
    "           .limit(count)\n",
    "           .selectExpr(\n",
    "               col,'HasDetections','HasDetectionsCount'\n",
    "           )).toPandas()\n",
    "    print(f\"Distinct values count: {col_distinct_count}\")\n",
    "    return pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_type(col):\n",
    "    d = dict(df.dtypes)\n",
    "    return d.get(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Plot:\n",
    "    \"\"\" \n",
    "    takes a col keyword for the column to plot, gets a pandas data frame and plots a sns catplot\n",
    "    of the top 40 col factors and their 'HasDetections' count\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, *args, **kwargs):\n",
    "        self.kwargs = {k:v for k,v in kwargs.items()}\n",
    "        self.col = kwargs.get('col')\n",
    "        self._plot(self._get_seaborn_pdf())\n",
    "        \n",
    "    def _get_seaborn_pdf(self):\n",
    "        pdf = (df.groupby(self.col, 'HasDetections')\n",
    "               .agg(F.count(self.col).alias(self.col+'Count'), F.count('HasDetections').alias('HasDetectionsCount'))\n",
    "               .sort(F.desc(self.col+'Count'))\n",
    "               .limit(40)\n",
    "               .selectExpr(\n",
    "                   self.col,self.col+'Count','HasDetections','HasDetectionsCount'\n",
    "               )).toPandas()\n",
    "        return pdf\n",
    "    \n",
    "    def _plot(self, pdf):\n",
    "        g = sns.catplot(x=self.col, y='HasDetectionsCount', data=pdf, kind='bar', hue='HasDetections')\n",
    "        g.set_xticklabels(rotation=90)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_hist_bar(_col):\n",
    "#     pdf = (df.select('HasDetections', _col).groupby(_col, 'HasDetections').count().sort(F.desc(_col)).limit(10)).toPandas()\n",
    "    pdf = pivot_plot(_col)\n",
    "    d = dict(df.dtypes)\n",
    "    if d[_col] == 'string' or d[_col] == 'int':\n",
    "        fig, ax1 = plt.subplots()\n",
    "        plt.xticks(rotation=90)\n",
    "        plt.title(_col + '_barplot')\n",
    "        ax1.bar(pdf[_col], pdf['HasDetectionsCount'])\n",
    "        ax2 = ax1.twinx()\n",
    "        ax2.plot(pdf[_col], pdf['HasDetectionsCount'], color='orange')\n",
    "        plt.show()\n",
    "    else:\n",
    "        fig, ax1 = plt.subplots()\n",
    "        plt.xticks(rotation=90)\n",
    "        plt.title(_col + '_barplot')\n",
    "        ax1.hist(x=pdf[_col], bins=30)\n",
    "#         ax2 = ax1.twinx()\n",
    "#         ax2.plot(pdf[_col], pdf['HasDetectionsCount'], color='orange')\n",
    "        plt.show()\n",
    "#         pdf.plot(kind='hist', x=_col, y='count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handle missing values\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert decimal dtypes to double"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decimal_dtypes = [col[0] for col in df.dtypes if 'decimal' in col[1]]\n",
    "decimal_dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (df.select('*')\n",
    "        .withColumn('Census_PrimaryDiskTotalCapacity', F.col('Census_PrimaryDiskTotalCapacity').cast('double'))\n",
    "        .withColumn('Census_InternalBatteryNumberOfCharges', F.col('Census_InternalBatteryNumberOfCharges').cast('double'))\n",
    "     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop columns with high occurence of null values > 70%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_df = df.select(\n",
    "    # get count of null values for each column\n",
    "    [(F.count(F.when(F.isnull(c), c))).alias(c) for c in df.columns]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create pandas dataframe: values are null counts of each column\n",
    "pdf = pd.DataFrame([_df.select(col).first() for col in _df.columns], _df.columns, columns=['count_null_values'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf = pdf.sort_values('count_null_values', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rows = df.count()\n",
    "pdf['null_ratio'] = pdf.count_null_values/num_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop_cols = ['PuaMode','Census_ProcessorClass','DefaultBrowsersIdentifier','Census_IsFlightingInternal','Census_InternalBatteryType','Census_ThresholdOptIn','Census_IsWIMBootEnabled']\n",
    "drop_cols = list(pdf[(pdf['null_ratio'] > .70)].index.values)\n",
    "drop_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(*drop_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Impute remaining missing values to -1 for numeric and '-1' for string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_num_cols(df)\n",
    "get_string_cols(df)\n",
    "df = df.fillna(-1, numeric_cols)\n",
    "df = df.fillna('-1', string_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split cols into numeric and string groups\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_num_cols(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_string_cols(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get skewness of columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df_skew = df.select(\n",
    "   [F.skewness(_col).alias(_col) for _col in df.columns]\n",
    ")\n",
    "pdf_skew = pd.DataFrame([df_skew.select(_col).first() for _col in df_skew.columns], df_skew.columns, columns=['skewness'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_skew.sort_values('skewness', ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize skewed features before dropping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = Plot(col='Census_PrimaryDiskTotalCapacity')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Maybe this could be a discriminating feature? But it's cardinality is very high--perhaps use hashing trick/binning to reduce dimensionality\n",
    "* Hashing/binning reduces model interpretability, perhaps use quartile transformation to maintain some sense of original unit?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_plot('Census_PrimaryDiskTotalCapacity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = Plot(col='UacLuaenable')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = Plot(col='SmartScreen')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* After reviewing the plot of this feature *SmartScreen*, decided not to drop it due to the potential discriminating effect at predicting P(Y|x)\n",
    "* TODO: write function to standardize values (e.g. off -> Off)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_plot('SmartScreen')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Plot(col='IsBeta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Plot(col='AutoSampleOptIn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop skewed columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skew_drop_cols = pdf_skew[pdf_skew['skewness'] > 100]\n",
    "skew_drop_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skew_drop_cols = [col for col in list(skew_drop_cols.index.values) if col not in ['SmartScreen', 'Census_PrimaryDiskTotalCapacity']]\n",
    "skew_drop_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(*skew_drop_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_num_cols(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_string_cols(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze distribution of columns by HasDetections column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Plot(col='CountryIdentifier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Plot(col='Census_OSVersion')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Plot(col='RtpStateBitfield')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* not sure why this feature didn't show as skewed in earlier analysis -- clearly it is skewed and doesn't appear to provide much predication power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_plot('RtpStateBitfield')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = Plot(col='EngineVersion')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = Plot(col='AppVersion')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = Plot(col='AvSigVersion')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* this feature looks like a good predictor of target class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "* [x] TODO: Standardize *SmartScreen* values\n",
    "* [x] TODO: Discretize (hashing trick/binning) of *Census_PrimaryDiskTotalCapacity*; decided to discretize based on quartile values\n",
    "* [x] TODO: Truncate EngineVersion [1.1.15100.1]\n",
    "* [x] TODO: Truncate AppVersion [4.18.1807.18075]\n",
    "* [ ] TODO: Truncate AvSigVersion [1.273.1735.0]\n",
    "* [ ] TODO: Discretize Census_SystemVolumeTotalCapacity [299451]\n",
    "* [ ] TODO: Discretize Census_InternalBatteryNumberOfCharges [4294967295.0]\n",
    "* [ ] TODO: Truncate AvSigVersion [1.273.1735.0]\n",
    "* [ ] TODO: Discretize Census_TotalPhysicalRAM [4096]\n",
    "* [ ] TODO: Discretize Census_InternalPrimaryDisplayResolutionHorizontal [1440]\n",
    "* [ ] TODO: Discretize Census_InternalPrimaryDisplayResolutionVertical [900]\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (df.select('*')\n",
    "      .withColumn('SmartScreen', F.regexp_replace('SmartScreen', r'off', 'Off'))\n",
    "      .withColumn('SmartScreen', F.regexp_replace('SmartScreen', r'OFF', 'Off'))\n",
    "      .withColumn('SmartScreen', F.regexp_replace('SmartScreen', r'on', 'On'))\n",
    "      .withColumn('SmartScreen', F.regexp_replace('SmartScreen', r'&#x02;', '2'))\n",
    "      .withColumn('SmartScreen', F.regexp_replace('SmartScreen', r'&#x01;', '1'))\n",
    "      .withColumn('SmartScreen', F.regexp_replace('SmartScreen', r'&#x03;', '3'))\n",
    "      .withColumn('SmartScreen', F.regexp_replace('SmartScreen', r'requireadmin', 'RequireAdmin'))\n",
    "      .withColumn('SmartScreen', F.regexp_replace('SmartScreen', r'Promt', 'Prompt'))\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_plot('SmartScreen', count=22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_plot('Census_PrimaryDiskTotalCapacity', count=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select('Census_PrimaryDiskTotalCapacity').summary().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create train, validation, test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_df, validation_df, testing_df = df.randomSplit([0.6, 0.3, 0.1], seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(featuresCol='feature', labelCol='HasDetections', maxDepth=6, numTrees=50, \n",
    "                            featureSubsetStrategy='sqrt', impurity='gini', seed=0)\n",
    "pipeline = Pipeline(stages=[\n",
    "    feature.StringIndexer(inputCol='OrganizationIdentifier',outputCol='str_idx'),\n",
    "    feature.OneHotEncoderEstimator(inputCols=['str_idx'], outputCols=['feature']),\n",
    "    rf\n",
    "    \n",
    "]).fit(training_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation.groupby('HasDetections').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.transform(validation_df).\\\n",
    "    select(F.avg(F.expr('float(HasDetections = prediction)')).alias('accuracy')).\\\n",
    "    first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncate(col):\n",
    "    print(f\"Original count of unique values: {df.select(col).distinct().count()}\")\n",
    "    fe_col_name = col + '_fe'\n",
    "    _df = (df.withColumn(fe_col_name, F.regexp_extract(col,r'\\d.\\d+.(\\d+.\\d+)',1))\n",
    "        .groupby(fe_col_name).count().sort(F.desc('count'))\n",
    "    )\n",
    "    Plot(col=col)\n",
    "    print(f\"Count of unique values: {_df.select(fe_col_name).distinct().count()}\")\n",
    "    pdf = _df.toPandas().loc[:40,]\n",
    "#     print(pdf.loc[:,fe_col_name].values)\n",
    "    f, ax = plt.subplots(figsize=(11, 9))\n",
    "#     g = (sns.catplot(x=fe_col_name, y='count', data=pdf, kind='bar', order=pdf.loc[:,fe_col_name].values)\n",
    "#          .set_xticklabels(labels=pdf.loc[:,fe_col_name].values,rotation=90)\n",
    "#         )\n",
    "    g = (sns.barplot(x=fe_col_name, y='count', data=pdf, order=pdf.loc[:,fe_col_name].values)\n",
    "     .set_xticklabels(labels=pdf.loc[:,fe_col_name].values, rotation=90)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "truncate('AvSigVersion')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "truncate('AppVersion')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_plot('AvSigVersion')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select('AvSigVersion').withColumn('AvSigVersion', F.regexp_extract('AvSigVersion',r'\\d.(\\d+.\\d+.\\d+)',1)).distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df.select('AvSigVersion')\n",
    " .withColumn('AvSigVersion', F.regexp_extract('AvSigVersion',r'\\d.(\\d+.\\d+.\\d+)',1))\n",
    " .groupby('AvSigVersion')\n",
    " .agg(F.count('AvSigVersion').alias('Count'))\n",
    " .sort(F.desc('Count'))\n",
    " .distinct()\n",
    " .count()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.select('*').withColumn('EngineVersion', F.regexp_extract('EngineVersion',r'\\d.\\d+.(\\d+.\\d+)',1))\n",
    "df = df.select('*').withColumn('AppVersion', F.regexp_extract('AppVersion',r'\\d.\\d+.(\\d+.\\d+)',1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@F.udf(returnType=T.StringType())\n",
    "def discretize_col(num):\n",
    "    # (953869.0, INF]\n",
    "    if num > 953869.0:\n",
    "        return 'x-large'\n",
    "    # (476940.0, 953869.0]\n",
    "    elif (num > 476940.0 and num <= 953869.0): \n",
    "        return 'med'\n",
    "    # (238475.0, 476940.0]\n",
    "    elif (num > 238475.0 and num <= 476940.0):\n",
    "        return 'small'\n",
    "    else:\n",
    "        return 'unknown'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (df.select('*')\n",
    "           .withColumn('Census_PrimaryDiskTotalCapacity', discretize_col(df.Census_PrimaryDiskTotalCapacity))\n",
    "     )\n",
    "df.groupby('Census_PrimaryDiskTotalCapacity').count().sort(F.desc('count')).show()\n",
    "\n",
    "# verify count \n",
    "# print('small', df.filter((df.Census_PrimaryDiskTotalCapacity > 238475.0) & (df.Census_PrimaryDiskTotalCapacity <= 476940.0)).count())\n",
    "# print('med', df.filter((df.Census_PrimaryDiskTotalCapacity > 476940.0) & (df.Census_PrimaryDiskTotalCapacity <= 953869.0)).count())\n",
    "# print('x-large', df.filter((df.Census_PrimaryDiskTotalCapacity > 953869.0)).count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Hashing Trick to deal with columns with high cardinality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.stat import ChiSquareTest\n",
    "\n",
    "def is_independent(cols):\n",
    "    va = VectorAssembler(inputCols=cols, outputCol='features')\n",
    "    _df = va.transform(df).select('features', 'HasDetections')\n",
    "    r = ChiSquareTest.test(_df, 'features', 'HasDetections').head()\n",
    "    print(\"pValues: \" + str(r.pValues))\n",
    "    print(\"degreesOfFreedom: \" + str(r.degreesOfFreedom))\n",
    "    print(\"statistics: \" + str(r.statistics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_distinct_counts = [(_col, df.select(_col).distinct().count()) for _col in numeric_cols + string_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_distinct_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_dimensional_cols_gt50 = [(_col, get_type(_col[0]))  for _col in col_distinct_counts if _col[1] > 50 and _col[0] not in ['MachineIdentifier', 'HasDetections']]\n",
    "low_dimensional_cols_lt50 = [(_col, get_type(_col[0])) for _col in col_distinct_counts if _col[1] < 50 and _col[0] not in ['MachineIdentifier', 'HasDetections']]\n",
    "high_dim_cols_int = [_col[0][0] for _col in high_dimensional_cols_gt50 if _col[0][0] not in ['MachineIdentifier', 'HasDetections'] and _col[1] == 'int']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoderEstimator\n",
    "help(OneHotEncoderEstimator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[col for col in high_dim_cols_int]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[col for col in high_dim_cols_int]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_inputCols = [col for col in high_dim_cols_int]\n",
    "_outputCols = [col + \"_vec\" for col in high_dim_cols_int]\n",
    "high_dimen_pipe = Pipeline(stages=[\n",
    "    feature.VectorAssembler(inputCols=_inputCols, outputCol='va_features'),\n",
    "#     feature.OneHotEncoderEstimator(inputCols=['va_features'], outputCols='features')\n",
    "]).fit(df).transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select('Census_SystemVolumeTotalCapacity').summary().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Plot(col='Census_SystemVolumeTotalCapacity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df.columns:\n",
    "    val = df.select(col).first()[col]\n",
    "    if val not in [0,1,-1]:\n",
    "        print(col, f\"[{df.select(col).first()[col]}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_plot('AvSigVersion')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(high_dimensional_cols_gt50, key=lambda x:x[0][1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_dimensional_cols_gt50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_dimensional_cols_lt50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_dim_cols_int = [_col[0][0] for _col in low_dimensional_cols_lt50 if _col[0][0] not in ['MachineIdentifier', 'HasDetections'] and _col[1] == 'int']\n",
    "low_dim_cols_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Plot(col='Census_IsFlightsDisabled')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Plot(col='HasTpm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Plot(col='AVProductsInstalled')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Plot(col='Census_IsSecureBootEnabled')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Plot(col='Census_IsPortableOperatingSystem')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import FeatureHasher\n",
    "\n",
    "hash_df = FeatureHasher(numFeatures=10000, inputCols=[col[0][0] for col in low_dimensional_cols_lt50], outputCol='hash_features')\n",
    "hash_data = hash_df.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hash_data.select('hash_features').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform ChiSquareTest to determine feature independence\n",
    "- (SPSS article) However, this does not mean the variables are strongly associated; a weak association in a large sample size may also result in p = 0.000\n",
    "- (machinelearningmastery.com) The variables are considered independent if the observed and expected frequencies are similar, that the levels of the variables do not interact, are not dependent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.stat import ChiSquareTest\n",
    "va = feature.VectorAssembler(inputCols=low_dim_cols_int, outputCol='features')\n",
    "va_df = va.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.stat import ChiSquareTest\n",
    "\n",
    "r = ChiSquareTest.test(va_df, \"features\", \"HasDetections\").head()\n",
    "print(\"pValues: \" + str(r.pValues))\n",
    "print(\"degreesOfFreedom: \" + str(r.degreesOfFreedom))\n",
    "print(\"statistics: \" + str(r.statistics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "chi_pdf = pd.DataFrame(r.statistics.toArray(), index=low_dim_cols_int, columns=['chi_stat']).sort_values('chi_stat',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = chi_pdf.index.to_list()\n",
    "f, ax = plt.subplots(figsize=(11, 9))\n",
    "g = (sns.barplot(x=cols, y='chi_stat',data=chi_pdf)\n",
    "     .set_xticklabels(labels=cols, rotation=90)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.stat import Correlation\n",
    "\n",
    "r1 = Correlation.corr(va_df, \"features\").collect()[0][0]\n",
    "# print(\"Pearson correlation matrix:\\n\" + str(r1[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrmatrix = r1.toArray().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_df = spark.createDataFrame(corrmatrix, low_dim_cols_int)\n",
    "corr_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_corr_pdf = corr_df.toPandas()\n",
    "pdf_corr_pdf.index = low_dim_cols_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a mask for the upper triangle\n",
    "mask = np.triu(np.ones_like(pdf_corr_pdf, dtype=np.bool))\n",
    "\n",
    "f, ax = plt.subplots(figsize=(11, 9))\n",
    "\n",
    "# Generate a custom diverging colormap\n",
    "cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "\n",
    "# Draw the heatmap with the mask and correct aspect ratio\n",
    "sns.heatmap(pdf_corr_pdf, mask=mask, cmap=cmap, vmax=.3, center=0,\n",
    "            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(11, 9))\n",
    "sns.heatmap(pdf_corr_pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO\n",
    "* correlation analysis\n",
    "* discretize numeric colums\n",
    "* discretize numeric fields using quartiles?\n",
    "* handle null values (drop/replace)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO create new dataframe with string indexed (StringIndexer) column of string columns with factors < 15\n",
    "### Factor columns will be vectorized and fed as input for model training induction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.stat import Correlation\n",
    "v = VectorAssembler(inputCols=['RtpStateBitfield', 'Census_OSInstallLanguageIdentifier'], outputCol='features').transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.stat import Correlation\n",
    "\n",
    "data = [(Vectors.sparse(4, [(0, 1.0), (3, -2.0)]),),\n",
    "        (Vectors.dense([4.0, 5.0, 0.0, 3.0]),),\n",
    "        (Vectors.dense([6.0, 7.0, 0.0, 8.0]),),\n",
    "        (Vectors.sparse(4, [(0, 9.0), (3, 1.0)]),)]\n",
    "_df_corr = spark.createDataFrame(data, [\"features\"])\n",
    "\n",
    "r1 = Correlation.corr(_df_corr, \"features\").head()\n",
    "print(\"Pearson correlation matrix:\\n\" + str(r1[0]))\n",
    "\n",
    "r2 = Correlation.corr(_df_corr, \"features\", \"spearman\").head()\n",
    "print(\"Spearman correlation matrix:\\n\" + str(r2[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "def _pipeline(_col):\n",
    "    string_output_col = _col + \"_index\"\n",
    "    oh_output_cols = _col + \"_vec\"\n",
    "    print(f\"Processing: {oh_output_cols}\")\n",
    "    pipeline = Pipeline(stages=[\n",
    "        StringIndexer(inputCol=_col, outputCol=string_output_col),\n",
    "        feature.OneHotEncoderEstimator(inputCols=[string_output_col], outputCols=[oh_output_cols])\n",
    "    ])\n",
    "\n",
    "    _df = pipeline.fit(df.select('MachineIdentifier',_col)).transform(df.select('MachineIdentifier',_col))\n",
    "    return _df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
