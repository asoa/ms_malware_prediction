{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment setup\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.conf import SparkConf\n",
    "from pyspark.storagelevel import StorageLevel\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.ml import feature, evaluation, Pipeline\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import re\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.conf.SparkConf at 0x7f66896d8e10>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf = (SparkConf()\n",
    "            .setAppName('ms_malware_pred')\n",
    "            .setMaster('spark://spark-master:7077')\n",
    "       )\n",
    "conf.set(\"spark.executor.cores\", \"2\")\n",
    "conf.set(\"spark.executor.memory\", \"16g\")\n",
    "conf.set(\"spark.driver.maxResultSize\", \"0\")\n",
    "conf.set(\"spark.default.parallelism\", \"6\")  # default number of cores in application\n",
    "conf.set(\"spark.driver.memory\", \"3g\") # may need to increase (from 3) due to list of fe dfs\n",
    "conf.set(\"spark.shuffle.file.buffer\", \"1m\")\n",
    "conf.set(\"spark.shuffle.unsafe.file.output.buffer\", \"1m\")\n",
    "conf.set(\"spark.sql.shuffle.partitions\", \"18\")  # 3 * cores available (6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.app.id', 'app-20200826105445-0003'),\n",
       " ('spark.sql.shuffle.partitions', '18'),\n",
       " ('spark.default.parallelism', '6'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.driver.host', '3ab0deefb9f3'),\n",
       " ('spark.driver.port', '39809'),\n",
       " ('spark.app.name', 'ms_malware_pred'),\n",
       " ('spark.executor.cores', '2'),\n",
       " ('spark.shuffle.unsafe.file.output.buffer', '1m'),\n",
       " ('spark.executor.memory', '16g'),\n",
       " ('spark.driver.memory', '3g'),\n",
       " ('spark.shuffle.file.buffer', '1m'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.master', 'spark://spark-master:7077'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.driver.maxResultSize', '0'),\n",
       " ('spark.submit.pyFiles', ''),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.ui.showConsoleProgress', 'true')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext.getConf().getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet('hdfs://namenode:9000/data/df_fe.parquet').orderBy(F.asc('MachineIdentifier')).repartition(18, 'MachineIdentifier')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = (spark.read.format('csv')\n",
    "#       .option('inferSchema', 'true')\n",
    "#       .option('header', 'true')\n",
    "#       .option('quote', '\\\"')\n",
    "#       .option('escape', '\\\"')\n",
    "#       .load('hdfs://namenode:9000/data/train.csv')\n",
    "#      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[MachineIdentifier: string, ProductName: string, EngineVersion: string, AppVersion: string, AvSigVersion: string, RtpStateBitfield: int, IsSxsPassiveMode: int, AVProductStatesIdentifier: int, AVProductsInstalled: int, AVProductsEnabled: int, HasTpm: int, CountryIdentifier: int, CityIdentifier: int, OrganizationIdentifier: int, GeoNameIdentifier: int, LocaleEnglishNameIdentifier: int, Platform: string, Processor: string, OsVer: string, OsBuild: int, OsSuite: int, OsPlatformSubRelease: string, OsBuildLab: string, SkuEdition: string, IsProtected: int, SMode: int, IeVerIdentifier: int, SmartScreen: string, Firewall: int, Census_MDC2FormFactor: string, Census_DeviceFamily: string, Census_OEMNameIdentifier: int, Census_OEMModelIdentifier: int, Census_ProcessorCoreCount: int, Census_ProcessorManufacturerIdentifier: int, Census_ProcessorModelIdentifier: int, Census_PrimaryDiskTotalCapacity: double, Census_PrimaryDiskTypeName: string, Census_SystemVolumeTotalCapacity: int, Census_HasOpticalDiskDrive: int, Census_TotalPhysicalRAM: int, Census_ChassisTypeName: string, Census_InternalPrimaryDiagonalDisplaySizeInInches: double, Census_InternalPrimaryDisplayResolutionHorizontal: int, Census_InternalPrimaryDisplayResolutionVertical: int, Census_PowerPlatformRoleName: string, Census_InternalBatteryNumberOfCharges: double, Census_OSVersion: string, Census_OSArchitecture: string, Census_OSBranch: string, Census_OSBuildNumber: int, Census_OSBuildRevision: int, Census_OSEdition: string, Census_OSSkuName: string, Census_OSInstallTypeName: string, Census_OSInstallLanguageIdentifier: int, Census_OSUILocaleIdentifier: int, Census_OSWUAutoUpdateOptionsName: string, Census_IsPortableOperatingSystem: int, Census_GenuineStateName: string, Census_ActivationChannel: string, Census_IsFlightsDisabled: int, Census_FlightRing: string, Census_ThresholdOptIn: int, Census_FirmwareManufacturerIdentifier: int, Census_FirmwareVersionIdentifier: int, Census_IsSecureBootEnabled: int, Census_IsWIMBootEnabled: int, Census_IsVirtualDevice: int, Census_IsTouchEnabled: int, Census_IsPenCapable: int, Census_IsAlwaysOnAlwaysConnectedCapable: int, Wdft_IsGamer: int, Wdft_RegionIdentifier: int, HasDetections: int]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.persist(StorageLevel.MEMORY_AND_DISK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8921483"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count() # execute the lazy DAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save as parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (df\n",
    "#  .coalesce(1)  # \n",
    "#  .write.format('parquet')\n",
    "#  .mode('overwrite')\n",
    "#  .save('hdfs://namenode:9000/data/df_fe.parquet')\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# added 'MachineIdentifier' in numeric list to enable common join \n",
    "numeric_cols = []\n",
    "def get_num_cols(_df):\n",
    "    global numeric_cols\n",
    "    numeric_cols = [x[0] for x in _df.dtypes if x[1] in ['int', 'double']]\n",
    "    return numeric_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_cols = []\n",
    "def get_string_cols(_df):\n",
    "    global string_cols\n",
    "    string_cols = [x[0] for x in _df.dtypes if x[1] == 'string']\n",
    "    return string_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_null_count(_df, _col):\n",
    "    # return count of null values in _col\n",
    "    df1 = _df.select(F.count(F.when(F.isnull(_col), _col)).alias('null_count'))\n",
    "    df2 = _df.groupby(_col).count().sort(F.desc('count'))\n",
    "    df1.show()\n",
    "    df2.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summ_stats(_col):\n",
    "    # return summary stats (e.g. min, max, mean, quantile for _col)\n",
    "    # this function isn't needed DF.summary() returns the same information faster\n",
    "    df.describe(_col).show()\n",
    "    print(df.approxQuantile(_col, [.25,.5,.75], 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pivot_plot(col, count=20):\n",
    "    col_distinct_count = df.select(col).distinct().count()\n",
    "    pdf = (df.groupby(col, 'HasDetections')\n",
    "           .agg(F.count(col).alias(col+'Count'), F.count('HasDetections').alias('HasDetectionsCount'))\n",
    "           .sort(F.desc(col+'Count'))\n",
    "           .limit(count)\n",
    "           .selectExpr(\n",
    "               col,'HasDetections','HasDetectionsCount'\n",
    "           )).toPandas()\n",
    "    print(f\"Distinct values count: {col_distinct_count}\")\n",
    "    return pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_type(col):\n",
    "    d = dict(df.dtypes)\n",
    "    return d.get(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Plot:\n",
    "    \"\"\" \n",
    "    takes a col keyword for the column to plot, gets a pandas data frame and plots a sns catplot\n",
    "    of the top 40 col factors and their 'HasDetections' count\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, *args, **kwargs):\n",
    "        self.kwargs = {k:v for k,v in kwargs.items()}\n",
    "        self.col = kwargs.get('col')\n",
    "        self._plot(self._get_seaborn_pdf())\n",
    "        \n",
    "    def _get_seaborn_pdf(self):\n",
    "        pdf = (df.groupby(self.col, 'HasDetections')\n",
    "               .agg(F.count(self.col).alias(self.col+'Count'), F.count('HasDetections').alias('HasDetectionsCount'))\n",
    "               .sort(F.desc(self.col+'Count'))\n",
    "               .limit(40)\n",
    "               .selectExpr(\n",
    "                   self.col,self.col+'Count','HasDetections','HasDetectionsCount'\n",
    "               )).toPandas()\n",
    "        return pdf\n",
    "    \n",
    "    def _plot(self, pdf):\n",
    "        g = sns.catplot(x=self.col, y='HasDetectionsCount', data=pdf, kind='bar', hue='HasDetections')\n",
    "        g.set_xticklabels(rotation=90)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_hist_bar(_col):\n",
    "#     pdf = (df.select('HasDetections', _col).groupby(_col, 'HasDetections').count().sort(F.desc(_col)).limit(10)).toPandas()\n",
    "    pdf = pivot_plot(_col)\n",
    "    d = dict(df.dtypes)\n",
    "    if d[_col] == 'string' or d[_col] == 'int':\n",
    "        fig, ax1 = plt.subplots()\n",
    "        plt.xticks(rotation=90)\n",
    "        plt.title(_col + '_barplot')\n",
    "        ax1.bar(pdf[_col], pdf['HasDetectionsCount'])\n",
    "        ax2 = ax1.twinx()\n",
    "        ax2.plot(pdf[_col], pdf['HasDetectionsCount'], color='orange')\n",
    "        plt.show()\n",
    "    else:\n",
    "        fig, ax1 = plt.subplots()\n",
    "        plt.xticks(rotation=90)\n",
    "        plt.title(_col + '_barplot')\n",
    "        ax1.hist(x=pdf[_col], bins=30)\n",
    "#         ax2 = ax1.twinx()\n",
    "#         ax2.plot(pdf[_col], pdf['HasDetectionsCount'], color='orange')\n",
    "        plt.show()\n",
    "#         pdf.plot(kind='hist', x=_col, y='count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert decimal dtypes to double"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decimal_dtypes = [col[0] for col in df.dtypes if 'decimal' in col[1]]\n",
    "decimal_dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (df.select('*')\n",
    "        .withColumn('Census_PrimaryDiskTotalCapacity', F.col('Census_PrimaryDiskTotalCapacity').cast('double'))\n",
    "        .withColumn('Census_InternalBatteryNumberOfCharges', F.col('Census_InternalBatteryNumberOfCharges').cast('double'))\n",
    "     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop columns with high occurence of null values > 70%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_df = df.select(\n",
    "    # get count of null values for each column\n",
    "    [(F.count(F.when(F.isnull(c), c))).alias(c) for c in df.columns]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create pandas dataframe: values are null counts of each column\n",
    "pdf = pd.DataFrame([_df.select(col).first() for col in _df.columns], _df.columns, columns=['count_null_values'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf = pdf.sort_values('count_null_values', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rows = df.count()\n",
    "pdf['null_ratio'] = pdf.count_null_values/num_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop_cols = ['PuaMode','Census_ProcessorClass','DefaultBrowsersIdentifier','Census_IsFlightingInternal','Census_InternalBatteryType','Census_ThresholdOptIn','Census_IsWIMBootEnabled']\n",
    "drop_cols = list(pdf[(pdf['null_ratio'] > .70)].index.values)\n",
    "drop_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(*drop_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impute remaining missing values to -1 for numeric and '-1' for string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_num_cols(df)\n",
    "get_string_cols(df)\n",
    "df = df.fillna(-1, numeric_cols)\n",
    "df = df.fillna('-1', string_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split cols into numeric and string groups\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_num_cols(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_string_cols(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get skewness of columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(F.skewness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df_skew = df.select(\n",
    "   [F.skewness(_col).alias(_col) for _col in df.columns]\n",
    ")\n",
    "pdf_skew = pd.DataFrame([df_skew.select(_col).first() for _col in df_skew.columns], df_skew.columns, columns=['skewness'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_skew.sort_values('skewness', ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize skewed features before dropping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = Plot(col='Census_PrimaryDiskTotalCapacity')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Maybe this could be a discriminating feature? But it's cardinality is very high--perhaps use hashing trick/binning to reduce dimensionality\n",
    "* Hashing/binning reduces model interpretability, perhaps use quartile transformation to maintain some sense of original unit?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = Plot(col='UacLuaenable')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = Plot(col='SmartScreen')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* After reviewing the plot of this feature *SmartScreen*, decided not to drop it due to the potential discriminating effect at predicting P(Y|x)\n",
    "* TODO: write function to standardize values (e.g. off -> Off)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_plot('SmartScreen')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Plot(col='IsBeta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Plot(col='AutoSampleOptIn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop skewed columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skew_drop_cols = pdf_skew[pdf_skew['skewness'] > 100]\n",
    "skew_drop_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skew_drop_cols = [col for col in list(skew_drop_cols.index.values) if col not in ['SmartScreen', 'Census_PrimaryDiskTotalCapacity']]\n",
    "skew_drop_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(*skew_drop_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_num_cols(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_string_cols(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze distribution of columns by HasDetections column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Plot(col='CountryIdentifier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Plot(col='Census_OSVersion')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Plot(col='RtpStateBitfield')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* not sure why this feature didn't show as skewed in earlier analysis -- clearly it is skewed and doesn't appear to provide much predication power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_plot('RtpStateBitfield')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = Plot(col='EngineVersion')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = Plot(col='AppVersion')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = Plot(col='AvSigVersion')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform ChiSquareTest to determine feature independence\n",
    "- (SPSS article) However, this does not mean the variables are strongly associated; a weak association in a large sample size may also result in p = 0.000\n",
    "- (machinelearningmastery.com) The variables are considered independent if the observed and expected frequencies are similar, that the levels of the variables do not interact, are not dependent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.stat import ChiSquareTest\n",
    "va = feature.VectorAssembler(inputCols=fe_df.columns, outputCol='features')\n",
    "va_df = va.transform(fe_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "va_df.select()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.stat import ChiSquareTest\n",
    "\n",
    "r = ChiSquareTest.test(va_df, \"features\", \"HasDetections\").head()\n",
    "print(\"pValues: \" + str(r.pValues))\n",
    "print(\"degreesOfFreedom: \" + str(r.degreesOfFreedom))\n",
    "print(\"statistics: \" + str(r.statistics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "chi_pdf = pd.DataFrame(r.statistics.toArray(), index=low_dim_cols_int, columns=['chi_stat']).sort_values('chi_stat',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = chi_pdf.index.to_list()\n",
    "f, ax = plt.subplots(figsize=(11, 9))\n",
    "g = (sns.barplot(x=cols, y='chi_stat',data=chi_pdf)\n",
    "     .set_xticklabels(labels=cols, rotation=90)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.stat import Correlation\n",
    "\n",
    "r1 = Correlation.corr(va_df, \"features\").collect()[0][0]\n",
    "# print(\"Pearson correlation matrix:\\n\" + str(r1[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrmatrix = r1.toArray().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_df = spark.createDataFrame(corrmatrix, low_dim_cols_int)\n",
    "corr_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_corr_pdf = corr_df.toPandas()\n",
    "pdf_corr_pdf.index = low_dim_cols_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a mask for the upper triangle\n",
    "mask = np.triu(np.ones_like(pdf_corr_pdf, dtype=np.bool))\n",
    "\n",
    "f, ax = plt.subplots(figsize=(11, 9))\n",
    "\n",
    "# Generate a custom diverging colormap\n",
    "cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "\n",
    "# Draw the heatmap with the mask and correct aspect ratio\n",
    "sns.heatmap(pdf_corr_pdf, mask=mask, cmap=cmap, vmax=.3, center=0,\n",
    "            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(11, 9))\n",
    "sns.heatmap(pdf_corr_pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* this feature looks like a good predictor of target class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "* [x] TODO: Standardize *SmartScreen* values\n",
    "* [x] TODO: Discretize (hashing trick/binning) of *Census_PrimaryDiskTotalCapacity*; decided to discretize based on quartile values\n",
    "* [x] TODO: Truncate EngineVersion [1.1.15100.1]\n",
    "* [x] TODO: Truncate AppVersion [4.18.1807.18075]\n",
    "* [ ] TODO: Truncate AvSigVersion [1.273.1735.0]\n",
    "* [x] TODO: Discretize Census_SystemVolumeTotalCapacity [299451]\n",
    "* [x] TODO: Discretize Census_InternalBatteryNumberOfCharges [4294967295.0]\n",
    "* [ ] TODO: Truncate AvSigVersion [1.273.1735.0]\n",
    "* [ ] TODO: Discretize Census_TotalPhysicalRAM [4096]\n",
    "* [ ] TODO: Discretize Census_InternalPrimaryDisplayResolutionHorizontal [1440]\n",
    "* [ ] TODO: Discretize Census_InternalPrimaryDisplayResolutionVertical [900]\n",
    "* [x] TODO: Onehot encode categorical data\n",
    "* [ ] TODO: Create hashing transformer function\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardize `SmartScreen` values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_fe = (df.select('*')\n",
    "      .withColumn('SmartScreen', F.regexp_replace('SmartScreen', r'off', 'Off'))\n",
    "      .withColumn('SmartScreen', F.regexp_replace('SmartScreen', r'OFF', 'Off'))\n",
    "      .withColumn('SmartScreen', F.regexp_replace('SmartScreen', r'on', 'On'))\n",
    "      .withColumn('SmartScreen', F.regexp_replace('SmartScreen', r'&#x02;', '2'))\n",
    "      .withColumn('SmartScreen', F.regexp_replace('SmartScreen', r'&#x01;', '1'))\n",
    "      .withColumn('SmartScreen', F.regexp_replace('SmartScreen', r'&#x03;', '3'))\n",
    "      .withColumn('SmartScreen', F.regexp_replace('SmartScreen', r'requireadmin', 'RequireAdmin'))\n",
    "      .withColumn('SmartScreen', F.regexp_replace('SmartScreen', r'Promt', 'Prompt'))\n",
    "     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discretize `Census_PrimaryDiskTotalCapacity`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_plot('Census_PrimaryDiskTotalCapacity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select('Census_PrimaryDiskTotalCapacity').summary().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@F.udf(returnType=T.StringType())\n",
    "def discretize_col(num):\n",
    "    # (953869.0, INF]\n",
    "    if num > 953869.0:\n",
    "        return 'large'\n",
    "    # (476940.0, 953869.0]\n",
    "    elif (num > 476940.0 and num <= 953869.0): \n",
    "        return 'med'\n",
    "    # (238475.0, 476940.0]\n",
    "    elif (num > 238475.0 and num <= 476940.0):\n",
    "        return 'small'\n",
    "    else:\n",
    "        return 'unknown'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (df.select('*')\n",
    "           .withColumn('Census_PrimaryDiskTotalCapacity', discretize_col(df.Census_PrimaryDiskTotalCapacity))\n",
    "     )\n",
    "df.groupby('Census_PrimaryDiskTotalCapacity').count().sort(F.desc('count')).show()\n",
    "\n",
    "# verify count \n",
    "# print('small', df.filter((df.Census_PrimaryDiskTotalCapacity > 238475.0) & (df.Census_PrimaryDiskTotalCapacity <= 476940.0)).count())\n",
    "# print('med', df.filter((df.Census_PrimaryDiskTotalCapacity > 476940.0) & (df.Census_PrimaryDiskTotalCapacity <= 953869.0)).count())\n",
    "# print('x-large', df.filter((df.Census_PrimaryDiskTotalCapacity > 953869.0)).count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Plot(col='Census_SystemVolumeTotalCapacity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select('Census_SystemVolumeTotalCapacity').summary().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "col_distinct_counts = [(_col, df.select(_col).distinct().count()) for _col in numeric_cols + string_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(col_distinct_counts, key=lambda col_distinct_counts: col_distinct_counts[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_num_cols(df)\n",
    "get_string_cols(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_plot('Census_SystemVolumeTotalCapacity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_plot('SmartScreen', count=22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_plot('Census_PrimaryDiskTotalCapacity', count=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fe.select('Census_PrimaryDiskTotalCapacity').summary().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncate(col):\n",
    "    print(f\"Original count of unique values: {df.select(col).distinct().count()}\")\n",
    "    fe_col_name = col + '_fe'\n",
    "    _df = (df.withColumn(fe_col_name, F.regexp_extract(col,r'\\d.(\\d+.\\d+).\\d+',1))\n",
    "        .groupby(fe_col_name).count().sort(F.desc('count'))\n",
    "    )\n",
    "    Plot(col=col)\n",
    "    print(f\"Count of unique values: {_df.select(fe_col_name).distinct().count()}\")\n",
    "    pdf = _df.toPandas().loc[:40,]\n",
    "#     print(pdf.loc[:,fe_col_name].values)\n",
    "    f, ax = plt.subplots(figsize=(11, 9))\n",
    "#     g = (sns.catplot(x=fe_col_name, y='count', data=pdf, kind='bar', order=pdf.loc[:,fe_col_name].values)\n",
    "#          .set_xticklabels(labels=pdf.loc[:,fe_col_name].values,rotation=90)\n",
    "#         )\n",
    "    g = (sns.barplot(x=fe_col_name, y='count', data=pdf, order=pdf.loc[:,fe_col_name].values)\n",
    "     .set_xticklabels(labels=pdf.loc[:,fe_col_name].values, rotation=90)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "truncate('AvSigVersion')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "truncate('AppVersion')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_plot('AvSigVersion')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select('AvSigVersion').withColumn('AvSigVersion', F.regexp_extract('AvSigVersion',r'\\d.(\\d+.\\d+.\\d+)',1)).distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df.select('AvSigVersion')\n",
    " .withColumn('AvSigVersion', F.regexp_extract('AvSigVersion',r'\\d.(\\d+.\\d+.\\d+)',1))\n",
    " .groupby('AvSigVersion')\n",
    " .agg(F.count('AvSigVersion').alias('Count'))\n",
    " .sort(F.desc('Count'))\n",
    " .distinct()\n",
    " .count()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.select('*').withColumn('EngineVersion', F.regexp_extract('EngineVersion',r'\\d.\\d+.(\\d+.\\d+)',1))\n",
    "df = df.select('*').withColumn('AppVersion', F.regexp_extract('AppVersion',r'\\d.\\d+.(\\d+.\\d+)',1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discretize `Census_SystemVolumeTotalCapacity`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@F.udf(returnType=T.StringType())\n",
    "def discretize_Census_SystemVolumeTotalCapacity(num):\n",
    "    # (475965.0, INF]\n",
    "    if num > 475965.0:\n",
    "        return 'large'\n",
    "    # (248951.0, 475965.0]\n",
    "    elif (num > 248951.0 and num <= 475965.0): \n",
    "        return 'med'\n",
    "    # (120487.0, 248951.0]\n",
    "    elif (num > 120487.0 and num <= 248951.0):\n",
    "        return 'small'\n",
    "    else:\n",
    "        return 'unknown'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (df.select('*')\n",
    "           .withColumn('Census_SystemVolumeTotalCapacity', discretize_Census_SystemVolumeTotalCapacity(df.Census_SystemVolumeTotalCapacity))\n",
    "     )\n",
    "df.groupby('Census_SystemVolumeTotalCapacity').count().sort(F.desc('count')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discretize `Census_InternalBatteryNumberOfCharges`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Plot(col='Census_InternalBatteryNumberOfCharges')\n",
    "# going to drop this col as it doesn't appear to have much predictive power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_plot(col='Census_InternalBatteryNumberOfCharges')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select('Census_InternalBatteryNumberOfCharges').summary().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Hashing Trick to deal with columns with high cardinality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import FeatureHasher\n",
    "\n",
    "hash_df = FeatureHasher(numFeatures=10000, inputCols=[col[0][0] for col in low_dimensional_cols_lt50], outputCol='hash_features')\n",
    "hash_data = hash_df.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hash_data.select('hash_features').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.stat import ChiSquareTest\n",
    "\n",
    "def is_independent(cols):\n",
    "    va = VectorAssembler(inputCols=cols, outputCol='features')\n",
    "    _df = va.transform(df).select('features', 'HasDetections')\n",
    "    r = ChiSquareTest.test(_df, 'features', 'HasDetections').head()\n",
    "    print(\"pValues: \" + str(r.pValues))\n",
    "    print(\"degreesOfFreedom: \" + str(r.degreesOfFreedom))\n",
    "    print(\"statistics: \" + str(r.statistics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_distinct_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_dimensional_cols_gt50 = [(_col, get_type(_col[0]))  for _col in col_distinct_counts if _col[1] > 50 and _col[0] not in ['MachineIdentifier', 'HasDetections']]\n",
    "low_dimensional_cols_lt50 = [(_col, get_type(_col[0])) for _col in col_distinct_counts if _col[1] < 50 and _col[0] not in ['MachineIdentifier', 'HasDetections']]\n",
    "high_dim_cols_int = [_col[0][0] for _col in high_dimensional_cols_gt50 if _col[0][0] not in ['MachineIdentifier', 'HasDetections'] and _col[1] == 'int']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_dimensional_cols_gt50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_inputCols = [col for col in high_dim_cols_int]\n",
    "_outputCols = [col + \"_vec\" for col in high_dim_cols_int]\n",
    "high_dimen_pipe = Pipeline(stages=[\n",
    "    feature.VectorAssembler(inputCols=_inputCols, outputCol='va_features'),\n",
    "#     feature.OneHotEncoderEstimator(inputCols=['va_features'], outputCols='features')\n",
    "]).fit(df).transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select('Census_SystemVolumeTotalCapacity').summary().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Plot(col='Census_SystemVolumeTotalCapacity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df.columns:\n",
    "    val = df.select(col).first()[col]\n",
    "    if val not in [0,1,-1]:\n",
    "        print(col, f\"[{df.select(col).first()[col]}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_plot('AvSigVersion')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(high_dimensional_cols_gt50, key=lambda x:x[0][1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_dimensional_cols_gt50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_dimensional_cols_lt50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_dim_cols_int = [_col[0][0] for _col in low_dimensional_cols_lt50 if _col[0][0] not in ['MachineIdentifier', 'HasDetections'] and _col[1] == 'int']\n",
    "low_dim_cols_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Plot(col='Census_IsFlightsDisabled')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Plot(col='HasTpm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Plot(col='AVProductsInstalled')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Plot(col='Census_IsSecureBootEnabled')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Plot(col='Census_IsPortableOperatingSystem')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index all features to numerical index\n",
    "indexers = [feature.StringIndexer(inputCol=col, outputCol=col+\"_index\").fit(df) for col in df.columns if col not in ['MachineIdentifier','HasDetections']]\n",
    "idx_df = Pipeline(stages=indexers).fit(df).transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encode features to vector\n",
    "input_cols = [col for col in idx_df.columns if 'index' in col]\n",
    "output_cols = [col + \"_vec\" for col in input_cols]\n",
    "one_hot = feature.OneHotEncoder(inputCols=input_cols, outputCols=output_cols).fit(idx_df).transform(idx_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assemble vectors into feature column\n",
    "va = feature.VectorAssembler(inputCols=[col for col in one_hot.columns if 'vec' in col], outputCol='features').transform(one_hot).select('MachineIdentifier','features','HasDetections')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[MachineIdentifier: string, features: vector, HasDetections: int]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "va.persist(StorageLevel.MEMORY_AND_DISK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8921483"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "va.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_df, validation_df, testing_df = fe_df.randomSplit([0.6, 0.3, 0.1], seed=0)\n",
    "training_df, validation_df, testing_df = va.randomSplit([0.6, 0.3, 0.1], seed=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logestic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "lr = LogisticRegression(featuresCol='features', labelCol='HasDetections')\n",
    "pipeline = Pipeline(stages=[\n",
    "#     feature.StringIndexer(inputCol='features',outputCol='str_idx', handleInvalid=\"keep\"),\n",
    "#     feature.OneHotEncoderEstimator(inputCols=['str_idx'], outputCols=['feature']),\n",
    "    lr\n",
    "    \n",
    "]).fit(training_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.transform(validation_df).\\\n",
    "    select(F.expr('float(prediction = HasDetections)').alias('correct')).\\\n",
    "    select(F.avg('correct')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.transform(testing_df).\\\n",
    "    select(F.expr('float(prediction = HasDetections)').alias('correct')).\\\n",
    "    select(F.avg('correct')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "rf = RandomForestClassifier(featuresCol='features', labelCol='HasDetections', maxDepth=3, numTrees=10, \n",
    "                            featureSubsetStrategy='sqrt', impurity='gini', seed=0, maxBins=32)\n",
    "pipeline = Pipeline(stages=[\n",
    "#     feature.StringIndexer(inputCol='features',outputCol='str_idx', handleInvalid=\"keep\"),\n",
    "#     feature.OneHotEncoderEstimator(inputCols=['str_idx'], outputCols=['feature']),\n",
    "    rf\n",
    "    \n",
    "]).fit(training_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.transform(validation_df).\\\n",
    "    select(F.avg(F.expr('float(HasDetections = prediction)')).alias('accuracy')).\\\n",
    "    first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.groupby('HasDetections').count().show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
