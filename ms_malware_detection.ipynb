{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary of Project\n",
    "* This notebook models data from https://www.kaggle.com/c/microsoft-malware-prediction/overview\n",
    "* Random Forest model generalizes better than the Logistic Regression with regularization; belive this is because of how bagging minimizes variance by aggregating weak predictors (trees) into a strong predictor\n",
    "* Believe the RF score can be improved by increasing the hashed feature space up from 200 (pyspark.ml.feature.FeatureHasher), grid-searching over the RF paramaters (numTrees/maxDepth) at the expense of higher computation costs and risk of over-fitting, and by removing features that don't have a relationship with the target -- using chi squared analyis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "---\n",
    "* [x] analyze correlation of features using chi-squared statistic\n",
    "* [x] reduce cardinality of features using discretization/hasing trick techniques\n",
    "* [x] grid search LR model regParam and elasticNet parameters \n",
    "* [x] create random forest\n",
    "* [x] grid search RF model\n",
    "* [ ] create gradient-boosted tree models\n",
    "* [x] enable Adaptive Query Execution to handle data skew, shuffle, and join logic\n",
    "* [x] fix driver.maxResultSize error\n",
    "* [x] create model with only features that show a relationship with target (lower scores --  < 100k?) from chi square statistic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment setup\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.conf import SparkConf\n",
    "from pyspark.storagelevel import StorageLevel\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.ml import feature, evaluation, Pipeline\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = (SparkConf()\n",
    "            .setAppName('ms_malware_pred')\n",
    "            .setMaster('spark://spark-master:7077')\n",
    "       )\n",
    "conf.set(\"spark.executor.cores\", \"2\")\n",
    "conf.set(\"spark.executor.memory\", \"16g\")\n",
    "conf.set(\"spark.driver.maxResultSize\", \"1g\")\n",
    "conf.set(\"spark.default.parallelism\", \"6\")  # default number of cores in application\n",
    "conf.set(\"spark.driver.memory\", \"3g\") \n",
    "conf.set(\"spark.shuffle.file.buffer\", \"1m\")\n",
    "conf.set(\"spark.shuffle.unsafe.file.output.buffer\", \"1m\")\n",
    "conf.set(\"spark.sql.shuffle.partitions\", \"18\")  # 3 * cores available (6)\n",
    "conf.set(\"spark.sql.adaptive.enabled\", \"true\") # enable Adaptive Query Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext.getConf().getAll()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_io(operation='clean'):\n",
    "    if operation == 'orig':\n",
    "        print('reading in original train.csv')\n",
    "        df = (spark.read.format('csv')\n",
    "          .option('inferSchema', 'true')\n",
    "          .option('header', 'true')\n",
    "          .option('quote', '\\\"')\n",
    "          .option('escape', '\\\"')\n",
    "          .load('hdfs://namenode:9000/data/train.csv') # train|test.csv\n",
    "         )\n",
    "        \n",
    "        (df\n",
    "         .coalesce(1) \n",
    "         .write.format('parquet')\n",
    "         .mode('overwrite')\n",
    "         .save('hdfs://namenode:9000/data/train.parquet')\n",
    "        )\n",
    "        \n",
    "    elif operation == 'clean':\n",
    "        print('reading in train.parquet')\n",
    "        df = spark.read.parquet('hdfs://namenode:9000/data/train.parquet')\n",
    "        df.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "        df.count()\n",
    "        return df\n",
    "    \n",
    "    elif operation == 'train':\n",
    "        print('reading in df_fe.parquet')\n",
    "        df_train = spark.read.parquet('hdfs://namenode:9000/data/df_fe.parquet')\n",
    "        df_train.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "        df_train.count()\n",
    "        return df_train\n",
    "    \n",
    "    else:\n",
    "        print('reading in df_test.parquet')\n",
    "        df_test = spark.read.parquet('hdfs://namenode:9000/data/df_test.parquet')\n",
    "        df_test.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "        df_test.count()\n",
    "        return df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_io(operation='orig')\n",
    "# df = file_io(operation='clean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_cols = []\n",
    "def get_num_cols(_df):\n",
    "    global numeric_cols\n",
    "    numeric_cols = [x[0] for x in _df.dtypes if x[1] in ['int', 'double']]\n",
    "    return numeric_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_cols = []\n",
    "def get_string_cols(_df):\n",
    "    global string_cols\n",
    "    string_cols = [x[0] for x in _df.dtypes if x[1] == 'string']\n",
    "    return string_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_null_count(_df, _col):\n",
    "    # return count of null values in _col\n",
    "    df1 = _df.select(F.count(F.when(F.isnull(_col), _col)).alias('null_count'))\n",
    "    df2 = _df.groupby(_col).count().sort(F.desc('count'))\n",
    "    df1.show()\n",
    "    df2.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pivot_plot(_df, col, count=20):\n",
    "    col_distinct_count = _df.select(col).distinct().count()\n",
    "    pdf = (_df.groupby(col, 'HasDetections')\n",
    "           .agg(F.count(col).alias(col+'Count'), F.count('HasDetections').alias('HasDetectionsCount'))\n",
    "           .sort(F.desc(col+'Count'))\n",
    "           .limit(count)\n",
    "           .selectExpr(\n",
    "               col,'HasDetections','HasDetectionsCount'\n",
    "           )).toPandas()\n",
    "    print(f\"Distinct values count: {col_distinct_count}\")\n",
    "    return pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_type(col):\n",
    "    d = dict(df.dtypes)\n",
    "    return d.get(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Plot:\n",
    "    \"\"\" \n",
    "    takes a col keyword for the column to plot, gets a pandas data frame and plots a sns catplot\n",
    "    of the top 40 col factors and their 'HasDetections' count\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, *args, **kwargs):\n",
    "        self.kwargs = {k:v for k,v in kwargs.items()}\n",
    "        self.col = kwargs.get('col')\n",
    "        self._df = kwargs.get('df')\n",
    "        self._plot(self._get_seaborn_pdf())\n",
    "        \n",
    "    def _get_seaborn_pdf(self):\n",
    "        pdf = (self._df.groupby(self.col, 'HasDetections')\n",
    "               .agg(F.count(self.col).alias(self.col+'Count'), F.count('HasDetections').alias('HasDetectionsCount'))\n",
    "               .sort(F.desc(self.col+'Count'))\n",
    "               .limit(40)\n",
    "               .selectExpr(\n",
    "                   self.col,self.col+'Count','HasDetections','HasDetectionsCount'\n",
    "               )).toPandas()\n",
    "        return pdf\n",
    "    \n",
    "    def _plot(self, pdf):\n",
    "        g = sns.catplot(x=self.col, y='HasDetectionsCount', data=pdf, kind='bar', hue='HasDetections')\n",
    "        g.set_xticklabels(rotation=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncate(col):\n",
    "    print(f\"Original count of unique values: {df.select(col).distinct().count()}\")\n",
    "    fe_col_name = col + '_fe'\n",
    "    _df = (df_fe.withColumn(fe_col_name, F.regexp_extract(col,r'\\d.\\d+.(\\d+).\\d+',1))  # change regex capture group to truncate field\n",
    "        .groupby(fe_col_name, 'HasDetections').count().sort(F.desc('count'))\n",
    "    )\n",
    "    Plot(col=col)\n",
    "    print(f\"Count of unique values: {_df.select(fe_col_name).distinct().count()}\")\n",
    "    pdf = _df.toPandas().loc[:40,]\n",
    "#     print(pdf.loc[:,fe_col_name].values)\n",
    "    f, ax = plt.subplots(figsize=(11, 9))\n",
    "#     g = (sns.catplot(x=fe_col_name, y='count', data=pdf, kind='bar', order=pdf.loc[:,fe_col_name].values)\n",
    "#          .set_xticklabels(labels=pdf.loc[:,fe_col_name].values,rotation=90)\n",
    "#         )\n",
    "    g = (sns.barplot(x=fe_col_name, y='count', hue='HasDetections', data=pdf, order=pdf.loc[:,fe_col_name].values)\n",
    "     .set_xticklabels(labels=pdf.loc[:,fe_col_name].values, rotation=90)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning/EDA\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop columns with high occurence of null values > 70%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_df = df.select(\n",
    "    # get count of null values for each column\n",
    "    [(F.count(F.when(F.isnull(c), c))).alias(c) for c in df.columns]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create pandas dataframe: values are null counts of each column\n",
    "pdf = pd.DataFrame([_df.select(col).first() for col in _df.columns], _df.columns, columns=['count_null_values'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf = pdf.sort_values('count_null_values', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rows = df.count()\n",
    "pdf['null_ratio'] = pdf.count_null_values/num_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop_cols = ['PuaMode','Census_ProcessorClass','DefaultBrowsersIdentifier','Census_IsFlightingInternal','Census_InternalBatteryType']\n",
    "drop_cols = list(pdf[(pdf['null_ratio'] > .70)].index.values)\n",
    "drop_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(*drop_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impute remaining missing values to -1 for numeric and '-1' for string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_num_cols(df)\n",
    "get_string_cols(df)\n",
    "df = df.fillna(-1, numeric_cols)\n",
    "df = df.fillna('-1', string_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split cols into numeric and string groups\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_num_cols(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_string_cols(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get skewness of columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df_skew = df.select(\n",
    "   [F.skewness(_col).alias(_col) for _col in df.columns]\n",
    ")\n",
    "pdf_skew = pd.DataFrame([df_skew.select(_col).first() for _col in df_skew.columns], df_skew.columns, columns=['skewness'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_skew.sort_values('skewness', ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize skewed features before dropping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = Plot(df=df, col='Census_PrimaryDiskTotalCapacity')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Maybe this could be a discriminating feature? But it's cardinality is very high--perhaps use hashing trick/binning to reduce dimensionality\n",
    "* Hashing/binning reduces model interpretability, perhaps use quartile transformation to maintain some sense of original unit?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = Plot(df=df, col='UacLuaenable')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = Plot(df=df,col='SmartScreen')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* After reviewing the plot of this feature *SmartScreen*, decided not to drop it due to the potential discriminating effect at predicting P(Y|x)\n",
    "* TODO: write function to standardize values (e.g. off -> Off)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Plot(df=df,col='IsBeta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Plot(df=df,col='AutoSampleOptIn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop skewed columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# skew_drop_cols = ['IsBeta', 'AutoSampleOptIn', 'UacLuaenable']\n",
    "skew_drop_cols = pdf_skew[pdf_skew['skewness'] > 100]\n",
    "skew_drop_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skew_drop_cols = [col for col in list(skew_drop_cols.index.values) if col not in ['SmartScreen', 'Census_PrimaryDiskTotalCapacity']]\n",
    "skew_drop_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(*skew_drop_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_num_cols(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_string_cols(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze distribution of columns by HasDetections column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Plot(df=df,col='CountryIdentifier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Plot(df=df,col='Census_OSVersion')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Plot(df=df,col='RtpStateBitfield')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* not sure why this feature didn't show as skewed in earlier analysis -- clearly it is skewed and doesn't appear to provide much predication power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = Plot(df=df,col='EngineVersion')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = Plot(df=df,col='AppVersion')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = Plot(df=df,col='AvSigVersion')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get relative frequency of largest factor for each categorical feature\n",
    "* drop columns where one factor is greater than 90% of column density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "def analyze_rel_freq():\n",
    "    cols = df.columns\n",
    "    rel_freqs = []\n",
    "    for col in cols:\n",
    "        rel_freqs.append((col, (df.groupby(col)\n",
    "         .count()\n",
    "         .withColumn('rel_freq', F.col('count')/df.count())\n",
    "         .orderBy(F.desc('count')).orderBy(F.desc('rel_freq'))\n",
    "        ).rdd.take(1)[0].rel_freq))\n",
    "    return rel_freqs\n",
    "rel_freqs = analyze_rel_freq()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_freqs = [('MachineIdentifier', 1.1208898789584646e-07), ('ProductName', 0.9893556934424468), ('EngineVersion', 0.43098966842171865), ('AppVersion', 0.5760504167300436), ('AvSigVersion', 0.011468608974539322), ('RtpStateBitfield', 0.969736421624073), ('IsSxsPassiveMode', 0.9826662226448226), ('AVProductStatesIdentifier', 0.652869595783571), ('AVProductsInstalled', 0.6959485323236059), ('AVProductsEnabled', 0.9700294222384328), ('HasTpm', 0.9879710581749693), ('CountryIdentifier', 0.04451860750056913), ('CityIdentifier', 0.0364747654621995), ('OrganizationIdentifier', 0.47037661787844015), ('GeoNameIdentifier', 0.17171237113829618), ('LocaleEnglishNameIdentifier', 0.23477991271182155), ('Platform', 0.9660630413127503), ('Processor', 0.9085300056055703), ('OsVer', 0.9676132320153499), ('OsBuild', 0.43888678597493264), ('OsSuite', 0.6232888635219055), ('OsPlatformSubRelease', 0.43888734641987215), ('OsBuildLab', 0.4100438234315976), ('SkuEdition', 0.6180969016025699), ('IsProtected', 0.9418032853954886), ('SMode', 0.9392881205960937), ('IeVerIdentifier', 0.4355600969031718), ('SmartScreen', 0.48379658404325826), ('Firewall', 0.9685625136538398), ('Census_MDC2FormFactor', 0.6415210341150681), ('Census_DeviceFamily', 0.998382555904663), ('Census_OEMNameIdentifier', 0.14428935189362577), ('Census_OEMModelIdentifier', 0.034162705908871875), ('Census_ProcessorCoreCount', 0.6086648374491102), ('Census_ProcessorManufacturerIdentifier', 0.8787012204136914), ('Census_ProcessorModelIdentifier', 0.03242543868547415), ('Census_PrimaryDiskTotalCapacity', 0.3185042217756846), ('Census_PrimaryDiskTypeName', 0.6508787832695528), ('Census_SystemVolumeTotalCapacity', 0.005940940536455655), ('Census_HasOpticalDiskDrive', 0.9228127207102227), ('Census_TotalPhysicalRAM', 0.4589497060073981), ('Census_ChassisTypeName', 0.5883340247355737), ('Census_InternalPrimaryDiagonalDisplaySizeInInches', 0.34158345647242727), ('Census_InternalPrimaryDisplayResolutionHorizontal', 0.5060889540449721), ('Census_InternalPrimaryDisplayResolutionVertical', 0.5574881440675278), ('Census_PowerPlatformRoleName', 0.6930358999731323), ('Census_InternalBatteryNumberOfCharges', 0.5664309397888221), ('Census_OSVersion', 0.15845201969224174), ('Census_OSArchitecture', 0.9085804456501234), ('Census_OSBranch', 0.449382462534536), ('Census_OSBuildNumber', 0.44935141388488886), ('Census_OSBuildRevision', 0.15845269222616912), ('Census_OSEdition', 0.3889477791976962), ('Census_OSSkuName', 0.38893410434117287), ('Census_OSInstallTypeName', 0.29233222772491974), ('Census_OSInstallLanguageIdentifier', 0.3563602598357246), ('Census_OSUILocaleIdentifier', 0.3554144529558595), ('Census_OSWUAutoUpdateOptionsName', 0.4432555663671612), ('Census_IsPortableOperatingSystem', 0.9994547991628746), ('Census_GenuineStateName', 0.8829918747813564), ('Census_ActivationChannel', 0.5299106661975369), ('Census_IsFlightsDisabled', 0.9819972755650602), ('Census_FlightRing', 0.9365796022925785), ('Census_ThresholdOptIn', 0.635244723326828), ('Census_FirmwareManufacturerIdentifier', 0.3025369212719455), ('Census_FirmwareVersionIdentifier', 0.017949145898725583), ('Census_IsSecureBootEnabled', 0.5139771044791545), ('Census_IsWIMBootEnabled', 0.6343903810610859), ('Census_IsVirtualDevice', 0.991184985724907), ('Census_IsTouchEnabled', 0.8744568587980271), ('Census_IsPenCapable', 0.9619290873501637), ('Census_IsAlwaysOnAlwaysConnectedCapable', 0.9350431985354901), ('Wdft_IsGamer', 0.6920534399942252), ('Wdft_RegionIdentifier', 0.2017719475562527), ('HasDetections', 0.5002073085831134)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_rel_freq_gt_90 = [x[0] for x in sorted(rel_freqs, key=lambda x: x[1], reverse=True) if x[1] > .9]\n",
    "drop_rel_freq_gt_90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(*drop_rel_freq_gt_90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "* [x] TODO: Discretize Census_SystemVolumeTotalCapacity [299451]\n",
    "* [-] TODO: Discretize Census_OEMModelIdentifier\n",
    "* [x] TODO: Standardize *SmartScreen* values\n",
    "* [x] TODO: Discretize (hashing trick/binning) of *Census_PrimaryDiskTotalCapacity*; decided to discretize based on quartile values\n",
    "* [x] TODO: Truncate EngineVersion [1.1.15100.1]\n",
    "* [x] TODO: Truncate AppVersion [4.18.1807.18075]\n",
    "* [x] TODO: Truncate AvSigVersion [1.273.1735.0]\n",
    "* [x] TODO: Discretize Census_InternalBatteryNumberOfCharges [4294967295.0]\n",
    "* [x] TODO: Discretize Census_TotalPhysicalRAM [4096]\n",
    "* [-] TODO: Discretize Census_InternalPrimaryDisplayResolutionHorizontal [1440]\n",
    "* [-] TODO: Discretize Census_InternalPrimaryDisplayResolutionVertical [900]\n",
    "* [x] TODO: Onehot encode categorical data\n",
    "* [x] TODO: Create hashing transformer function\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Cardinality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from pyspark.sql.functions import approx_count_distinct\n",
    "# col_distinct_counts = [(_col, df_fe.select(_col).distinct().count()) for _col in numeric_cols + string_cols]\n",
    "col_distinct_counts = sorted([(_col, df.select(approx_count_distinct(_col, .1)).rdd.take(1)[0][0]) for _col in df.columns], key=lambda x: x[1], reverse=True)\n",
    "col_distinct_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fe = df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discretize `Census_SystemVolumeTotalCapacity`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fe.select('Census_SystemVolumeTotalCapacity').summary().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@F.udf(returnType=T.StringType())\n",
    "def discretize_Census_SystemVolumeTotalCapacity(num):\n",
    "    # (475965.0, INF]\n",
    "    if num > 475798.0:\n",
    "        return 'large'\n",
    "    # (239500.0, 475965.0]\n",
    "    elif (num > 239500.0 and num <= 475965.0): \n",
    "        return 'med'\n",
    "    # (113922.0, 239500.0]\n",
    "    elif (num > 113922.0 and num <= 239500.0):\n",
    "        return 'small'\n",
    "    else:\n",
    "        return 'unknown'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fe = (df_fe.select('*')\n",
    "           .withColumn('Census_SystemVolumeTotalCapacity', discretize_Census_SystemVolumeTotalCapacity(df.Census_SystemVolumeTotalCapacity))\n",
    "     )\n",
    "df_fe.groupby('Census_SystemVolumeTotalCapacity').count().sort(F.desc('count')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discretize `Census_OEMModelIdentifier`\n",
    "* don't currently see a way to discretize this feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Plot(df=df,col='Census_OEMModelIdentifier')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardize `SmartScreen` values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def descretize_smart_screen(df):\n",
    "    df = (df.withColumn('SmartScreen', F.regexp_replace('SmartScreen', r'off', 'Off'))\n",
    "      .withColumn('SmartScreen', F.regexp_replace('SmartScreen', r'OFF', 'Off'))\n",
    "      .withColumn('SmartScreen', F.regexp_replace('SmartScreen', r'on', 'On'))\n",
    "      .withColumn('SmartScreen', F.regexp_replace('SmartScreen', r'&#x02;', '2'))\n",
    "      .withColumn('SmartScreen', F.regexp_replace('SmartScreen', r'&#x01;', '1'))\n",
    "      .withColumn('SmartScreen', F.regexp_replace('SmartScreen', r'&#x03;', '3'))\n",
    "      .withColumn('SmartScreen', F.regexp_replace('SmartScreen', r'requireadmin', 'RequireAdmin'))\n",
    "      .withColumn('SmartScreen', F.regexp_replace('SmartScreen', r'Promt', 'Prompt'))\n",
    "     )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fe = descretize_smart_screen(df_fe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discretize `Census_PrimaryDiskTotalCapacity`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fe.select('Census_PrimaryDiskTotalCapacity').summary().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@F.udf(returnType=T.StringType())\n",
    "def discretize_primary_disk_total_capacity(num):\n",
    "    # (953869.0, INF]\n",
    "    if num > 953869.0:\n",
    "        return 'large'\n",
    "    # (476940.0, 953869.0]\n",
    "    elif (num > 476940.0 and num <= 953869.0): \n",
    "        return 'med'\n",
    "    # (238475.0, 476940.0]\n",
    "    elif (num > 238475.0 and num <= 476940.0):\n",
    "        return 'small'\n",
    "    else:\n",
    "        return 'unknown'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fe = (df_fe.select('*')\n",
    "           .withColumn('Census_PrimaryDiskTotalCapacity', discretize_primary_disk_total_capacity(df.Census_PrimaryDiskTotalCapacity))\n",
    "     )\n",
    "df_fe.groupby('Census_PrimaryDiskTotalCapacity').count().sort(F.desc('count')).show()\n",
    "\n",
    "# verify count \n",
    "# print('small', df.filter((df.Census_PrimaryDiskTotalCapacity > 238475.0) & (df.Census_PrimaryDiskTotalCapacity <= 476940.0)).count())\n",
    "# print('med', df.filter((df.Census_PrimaryDiskTotalCapacity > 476940.0) & (df.Census_PrimaryDiskTotalCapacity <= 953869.0)).count())\n",
    "# print('x-large', df.filter((df.Census_PrimaryDiskTotalCapacity > 953869.0)).count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Plot(df=df_fe,col='Census_SystemVolumeTotalCapacity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_num_cols(df)\n",
    "get_string_cols(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discretize `EngineVersion` `AppVersion` `AvSigVersion`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fe = df_fe.select('*').withColumn('EngineVersion', F.regexp_extract('EngineVersion',r'\\d.\\d+.(\\d+.\\d+)',1))\n",
    "df_fe = df_fe.select('*').withColumn('AppVersion', F.regexp_extract('AppVersion',r'\\d.\\d+.(\\d+.\\d+)',1))\n",
    "df_fe = df_fe.select('*').withColumn('AvSigVersion', F.regexp_extract('AvSigVersion',r'\\d.\\d+.(\\d+).\\d+',1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discretize `Census_InternalBatteryNumberOfCharges`\n",
    "* dropped feature because of lack of variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fe = (df_fe.select('*')\n",
    "         .withColumn('Census_InternalBatteryNumberOfCharges', (F.col('Census_InternalBatteryNumberOfCharges').cast('int')))\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fe.select('Census_InternalBatteryNumberOfCharges').summary().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Plot(df=df_fe,col='Census_InternalBatteryNumberOfCharges')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fe = df_fe.drop('Census_InternalBatteryNumberOfCharges')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discretize `Census_TotalPhysicalRAM`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Plot(df=df_fe,col='Census_TotalPhysicalRAM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fe.select('Census_TotalPhysicalRAM').summary().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize ram by 1024 to reduce cardinality\n",
    "df_fe = (df_fe.select('*')\n",
    "         .withColumn('Census_TotalPhysicalRAM', F.col('Census_TotalPhysicalRAM')/1024)\n",
    "         .withColumn('Census_TotalPhysicalRAM', F.round(F.col('Census_TotalPhysicalRAM'),1))\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Plot(df=df_fe, col='Census_TotalPhysicalRAM')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discretize `Census_InternalPrimaryDisplayResolutionHorizontal|Vertical`\n",
    "* decided to keep feauture as-is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Plot(df=df_fe, col='Census_InternalPrimaryDisplayResolutionHorizontal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Plot(df=df_fe, col='Census_InternalPrimaryDisplayResolutionVertical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_df = (df_fe\n",
    " .withColumn('screen_dims', F.concat(df_fe.Census_InternalPrimaryDisplayResolutionHorizontal, F.lit('*'), df_fe.Census_InternalPrimaryDisplayResolutionVertical))\n",
    " .groupby('screen_dims')\n",
    " .count()\n",
    " .orderBy(F.desc('count'))\n",
    " .show()\n",
    "#  .select('screen_dims').show()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_fe = df_fe.withColumn('screen_dims', F.concat(df_fe.Census_InternalPrimaryDisplayResolutionHorizontal, F.lit('*'), df_fe.Census_InternalPrimaryDisplayResolutionVertical))\n",
    "# df_fe = df_fe.drop('Census_InternalPrimaryDisplayResolutionHorizontal', 'Census_InternalPrimaryDisplayResolutionVertical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fe.persist(StorageLevel.MEMORY_AND_DISK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fe.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write cleaned/FE DF to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (df_fe\n",
    "#  .coalesce(1) \n",
    "#  .write.format('parquet')\n",
    "#  .mode('overwrite')\n",
    "#  .save('hdfs://namenode:9000/data/df_fe.parquet')\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read-in cleaned train data\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = file_io(operation='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = file_io(operation='test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reconcile train/test columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_drop_cols = [x for x in (set(df_test.columns) - set(df_train.columns)) if x != 'HasDetections']\n",
    "test_drop_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df_test.drop(*test_drop_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_drop_cols = [x for x in (set(df_train.columns) - set(df_test.columns)) if x != 'HasDetections']\n",
    "train_drop_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.drop(*train_drop_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df_train.columns), len(df_test.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ChiSquareTest feature selection\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fe = df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "fe_distinct_count = sorted([(_col, df_fe.select(F.approx_count_distinct(_col, .1)).rdd.take(1)[0][0]) for _col in df_fe.columns], key=lambda x: x[1], reverse=True)\n",
    "fe_distinct_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chi test statistic algorithm seems to not handle features with cardinality > 10000\n",
    "cols = [x[0] for x in fe_distinct_count if x[1] < 10000 and x[0] not in ['MachineIdentifier','HasDetections']] \n",
    "# cols = [x[0] for x in fe_distinct_count if x[0] not in ['MachineIdentifier','HasDetections']] \n",
    "cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chi_test_df = spark.read.parquet('hdfs://namenode:9000/data/df_fe.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexer = feature.StringIndexer(inputCols=cols, outputCols=[col + \"_index\" for col in cols])\n",
    "va = feature.VectorAssembler(inputCols=indexer.getOutputCols(), outputCol='features')\n",
    "\n",
    "def chi_test_pipeline(df):\n",
    "    transformed_df = Pipeline(stages=[\n",
    "        indexer,\n",
    "        va\n",
    "    ]).fit(df).transform(df).select('features','HasDetections')\n",
    "    return transformed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi_test_df = chi_test_pipeline(df_fe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi_test_df.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "chi_test_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.stat import ChiSquareTest\n",
    "\n",
    "\n",
    "r = ChiSquareTest.test(chi_test_df, \"features\", \"HasDetections\").head()\n",
    "print(\"pValues: \" + str(r.pValues))\n",
    "print(\"degreesOfFreedom: \" + str(r.degreesOfFreedom))\n",
    "print(\"statistics: \" + str(r.statistics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "chi_pdf = pd.DataFrame(r.statistics.toArray(), index=cols, columns=['chi_stat']).sort_values('chi_stat',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(11, 9))\n",
    "g = (sns.barplot(x='chi_stat', y=chi_pdf.index,data=chi_pdf)\n",
    "#      .set_xticklabels(labels='chi_stat', rotation=90)     \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select features that have a relationship with target; cut off at EngineVersion as observed versus expected values diverge exponentially from there \n",
    "chi_stat_relationship_cols = chi_pdf.loc[chi_pdf['chi_stat'] < 144878, 'chi_stat'].index.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi_pdf.sort_values('chi_stat', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select features that have a relationship with the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi_stat_relationship_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi_keep_cols = list(set(df_train.columns).intersection(chi_stat_relationship_cols)) + ['MachineIdentifier','HasDetections']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi_keep_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(cols) - set(chi_keep_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.select(chi_keep_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df_test.select([col for col in chi_keep_cols if col != 'HasDetections'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df_train.columns), len(df_test.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hashed_df_train, hashed_df_validation = hashed_df.randomSplit([.9, .1], seed=0)\n",
    "hashed_df_train, hashed_df_validation = df_train.randomSplit([.9, .1], seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashed_df_train.persist(StorageLevel.MEMORY_ONLY)\n",
    "hashed_df_train.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashed_df_validation.persist(StorageLevel.MEMORY_ONLY)\n",
    "hashed_df_validation.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation Analaysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.stat import Correlation\n",
    "\n",
    "r1 = Correlation.corr(chi_test_df, \"features\").collect()[0][0]\n",
    "# print(\"Pearson correlation matrix:\\n\" + str(r1[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrmatrix = r1.toArray().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_df = spark.createDataFrame(corrmatrix, cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_corr_pdf = corr_df.toPandas()\n",
    "pdf_corr_pdf.index = corr_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From https://seaborn.pydata.org/examples/many_pairwise_correlations.html\n",
    "\n",
    "# Generate a mask for the upper triangle\n",
    "mask = np.triu(np.ones_like(pdf_corr_pdf, dtype=np.bool))\n",
    "\n",
    "f, ax = plt.subplots(figsize=(20,18))\n",
    "\n",
    "# Generate a custom diverging colormap\n",
    "cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "\n",
    "# Draw the heatmap with the mask and correct aspect ratio\n",
    "sns.heatmap(pdf_corr_pdf, mask=mask, cmap=cmap, vmax=.3, center=0,\n",
    "            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi_test_df.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [col for col in df_train.columns if x not in ['MachineIdentifier','HasDetections']] \n",
    "# cols = [col for col in df_train.columns if x not in ['MachineIdentifier','HasDetections'] + chi_stat_relationship_cols]\n",
    "\n",
    "def modeling_pipeline(df, test=False):\n",
    "    cols = [col for col in df.columns if col not in ['MachineIdentifier','HasDetections']]    \n",
    "    indexer = feature.StringIndexer(inputCols=cols, outputCols=[col + \"_index\" for col in cols])\n",
    "    one_hot = feature.OneHotEncoder(inputCols=indexer.getOutputCols(), outputCols=[col + \"_oh\" for col in cols])\n",
    "    va = feature.VectorAssembler(inputCols=indexer.getOutputCols(), outputCol='features')\n",
    "    \n",
    "    fitted_df = Pipeline(stages=[\n",
    "        indexer,\n",
    "        one_hot,\n",
    "        va\n",
    "    ]).fit(df)\n",
    "    \n",
    "    if test:\n",
    "        transformed_df = fitted_df.transform(df).select('MachineIdentifier','features')\n",
    "    else:\n",
    "        transformed_df = fitted_df.transform(df).select('MachineIdentifier','features','HasDetections')\n",
    "    return transformed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_df_fe = modeling_pipeline(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_df, validation_df = training_df_fe.randomSplit([0.9, 0.1], seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_df.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "training_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_df.persist(StorageLevel.MEMORY_ONLY)\n",
    "validation_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logestic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "hasher_cols = [col for col in chi_keep_cols if col not in ['MachineIdentifier','HasDetections']]\n",
    "\n",
    "hasher = feature.FeatureHasher(numFeatures=512,inputCols=hasher_cols,outputCol='hash_features', categoricalCols=hasher_cols)\n",
    "lr = LogisticRegression(featuresCol='hash_features', labelCol='label')\n",
    "\n",
    "pipeline = Pipeline(stages=[\n",
    "    hasher,\n",
    "    lr\n",
    "]).fit(hashed_df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.transform(hashed_df_validation).\\\n",
    "    select(F.expr('float(prediction = HasDetections)').alias('correct')).\\\n",
    "    select(F.avg('correct')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LR Model Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Loop over each parameter mapping in paramGrid and fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# hashed_df_train = hashed_df_train.withColumnRenamed('HasDetections', 'label')\n",
    "df_train = df_train.withColumnRenamed('HasDetections', 'label')\n",
    "# hasher_cols = [col for col in chi_keep_cols if col not in ['MachineIdentifier','HasDetections']]\n",
    "cols = [col for col in chi_keep_cols if col not in ['MachineIdentifier','HasDetections']]    \n",
    "\n",
    "\n",
    "# transformers\n",
    "# hasher = feature.FeatureHasher(inputCols=hasher_cols,outputCol='hash_features', categoricalCols=hasher_cols)\n",
    "indexer = feature.StringIndexer(inputCols=cols, outputCols=[col + \"_index\" for col in cols], handleInvalid='keep')\n",
    "one_hot = feature.OneHotEncoder(inputCols=indexer.getOutputCols(), outputCols=[col + \"_oh\" for col in cols])\n",
    "va = feature.VectorAssembler(inputCols=one_hot.getOutputCols(), outputCol='features')\n",
    "lr = LogisticRegression(featuresCol='features', labelCol='label')\n",
    "\n",
    "_evaluator = BinaryClassificationEvaluator(labelCol='label', metricName='areaUnderROC')\n",
    "\n",
    "paramGrid = (ParamGridBuilder() \n",
    "#                  .addGrid(hasher.numFeatures, [256,512])\n",
    "#                  .addGrid(lr.regParam, [0.1, 0.01]) \n",
    "                 .addGrid(lr.elasticNetParam, [0.2, 0.5, 0.8]) \n",
    "                 .build()\n",
    "            )\n",
    "\n",
    "pipeline = Pipeline(stages=[\n",
    "#     hasher,\n",
    "    indexer,\n",
    "    one_hot,\n",
    "    va,\n",
    "    lr\n",
    "])\n",
    "\n",
    "tvs = TrainValidationSplit(estimator=pipeline,\n",
    "                           estimatorParamMaps=paramGrid,\n",
    "                           evaluator=_evaluator,\n",
    "                           trainRatio=0.8)\n",
    "\n",
    "lr_model = tvs.fit(df_train)\n",
    "# lr_model = tvs.fit(hashed_df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "# evaluator = BinaryClassificationEvaluator(labelCol='HasDetections', metricName='areaUnderROC')\n",
    "# lr_auc_scores = [evaluator.evaluate(model.transform(hashed_df_validation)) for model in lr_models]\n",
    "lr_model.bestModel.transform(hashed_df_validation).\\\n",
    "    select(F.expr('float(prediction = HasDetections)').alias('correct')).\\\n",
    "    select(F.avg('correct')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df_test.withColumnRenamed('HasDetections', 'label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_predict = lr_model.bestModel.transform(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submit = df_test_predict.select('MachineIdentifier', 'prediction').withColumnRenamed('prediction','HasDetections')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submit.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df_submit\n",
    " .coalesce(1) \n",
    " .write.format('csv')\n",
    " .mode('overwrite')\n",
    " .save('hdfs://namenode:9000/data/lr_en_submit.csv')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = [x.name for x in list(lr_model.bestModel.stages[3].extractParamMap().keys())]\n",
    "values = list(lr_model.bestModel.stages[3].extractParamMap().values())\n",
    "list(zip(keys,values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LR Model with regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "hasher = feature.FeatureHasher(numFeatures=512,inputCols=hasher_cols,outputCol='hash_features', categoricalCols=hasher_cols)\n",
    "lr = LogisticRegression(featuresCol='hash_features', labelCol='label', regParam=0.0, elasticNetParam=0.2)\n",
    "reg_pipeline = Pipeline(stages=[\n",
    "    hasher,\n",
    "    lr\n",
    "]).fit(hashed_df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_pipeline.transform(hashed_df_validation).\\\n",
    "    select(F.expr('float(prediction = HasDetections)').alias('correct')).\\\n",
    "    select(F.avg('correct')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean/FE test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_fe_test_dataset(df):\n",
    "#     if 'Census_SystemVolumeTotalCapacity' in df.columns:\n",
    "#         df = df.withColumn('Census_SystemVolumeTotalCapacity', discretize_Census_SystemVolumeTotalCapacity(df.Census_SystemVolumeTotalCapacity))\n",
    "#     if 'SmartScreen' in df.columns:\n",
    "#         df = descretize_smart_screen(df)\n",
    "#     if 'Census_PrimaryDiskTotalCapacity' in df.columns:\n",
    "#         df = df.withColumn('Census_PrimaryDiskTotalCapacity', discretize_primary_disk_total_capacity(df.Census_PrimaryDiskTotalCapacity))\n",
    "#     if 'EngineVersion' in df.columns:\n",
    "#         df = df.withColumn('EngineVersion', F.regexp_extract('EngineVersion',r'\\d.\\d+.(\\d+.\\d+)',1))\n",
    "    if 'AppVersion' in df.columns:\n",
    "        df = df.withColumn('AppVersion', F.regexp_extract('AppVersion',r'\\d.\\d+.(\\d+.\\d+)',1))\n",
    "    if 'AvSigVersion' in df.columns:\n",
    "        df = df.withColumn('AvSigVersion', F.regexp_extract('AvSigVersion',r'\\d.\\d+.(\\d+).\\d+',1))\n",
    "    if 'Census_InternalBatteryNumberOfCharges' in df.columns:\n",
    "        df = df.drop('Census_InternalBatteryNumberOfCharges')\n",
    "    if 'Census_TotalPhysicalRAM' in df.columns:\n",
    "        df = (df.withColumn('Census_TotalPhysicalRAM', F.col('Census_TotalPhysicalRAM')/1024)\n",
    "              .withColumn('Census_TotalPhysicalRAM', F.round(F.col('Census_TotalPhysicalRAM'),1))\n",
    "             )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_test = spark.read.parquet('hdfs://namenode:9000/data/df_test.parquet')\n",
    "# df_test.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "# df_test.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_fe = clean_fe_test_dataset(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_fe = modeling_pipeline(df_test_fe, test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_fe.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "df_test_fe.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make predictions on test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_predict = pipeline.transform(df_test_fe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_predict.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submit = df_test_predict.select('MachineIdentifier', 'prediction').withColumnRenamed('prediction','HasDetections')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submit.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (df_submit\n",
    "#  .coalesce(1) \n",
    "#  .write.format('csv')\n",
    "#  .mode('overwrite')\n",
    "#  .save('hdfs://namenode:9000/data/df_submit.csv')\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest\n",
    "* OOM errors; believe these are occuring because of high cardinality of features\n",
    "* [x] Plan to use feature hasher to reduce cardinality\n",
    "* use this code block for RF model with chi square features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "hashed_df_train = hashed_df_train.withColumnRenamed('HasDetections', 'label')  # rf looks for label column\n",
    "hasher_cols = [col for col in chi_keep_cols if col not in ['MachineIdentifier','HasDetections']]\n",
    "\n",
    "hasher = feature.FeatureHasher(numFeatures=512,inputCols=hasher_cols,outputCol='hash_features', categoricalCols=hasher_cols)\n",
    "rf = RandomForestClassifier(featuresCol='hash_features', labelCol='label', maxDepth=15,\\\n",
    "                            numTrees=200, featureSubsetStrategy='sqrt', impurity='gini', seed=0)\n",
    "rf_pipeline = Pipeline(stages=[\n",
    "    hasher,\n",
    "    rf\n",
    "]).fit(hashed_df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_pipeline.transform(hashed_df_validation).\\\n",
    "    select(F.avg(F.expr('float(HasDetections = prediction)')).alias('accuracy')).\\\n",
    "    first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RF Model Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "hashed_df_train = hashed_df_train.withColumnRenamed('HasDetections', 'label')\n",
    "hasher_cols = [col for col in chi_keep_cols if col not in ['MachineIdentifier','HasDetections']]\n",
    "\n",
    "# transformers\n",
    "hasher = feature.FeatureHasher(inputCols=hasher_cols,outputCol='hash_features', categoricalCols=hasher_cols)\n",
    "rf = RandomForestClassifier(featuresCol='hash_features', labelCol='label', featureSubsetStrategy='sqrt', impurity='gini', seed=0)\n",
    "\n",
    "_evaluator = BinaryClassificationEvaluator(labelCol='label', metricName='areaUnderROC')\n",
    "\n",
    "paramGrid = (ParamGridBuilder() \n",
    "                .addGrid(hasher.numFeatures, [1024]) \n",
    "                .addGrid(rf.numTrees, [150]) \n",
    "                .addGrid(rf.maxDepth, [15]) \n",
    "                .build()\n",
    "            )\n",
    "\n",
    "rf_pipeline = Pipeline(stages=[\n",
    "    hasher,\n",
    "    rf\n",
    "])\n",
    "\n",
    "tvs = TrainValidationSplit(estimator=rf_pipeline,\n",
    "                           estimatorParamMaps=paramGrid,\n",
    "                           evaluator=_evaluator,\n",
    "                           trainRatio=0.8)\n",
    "\n",
    "rf_model = tvs.fit(hashed_df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model.bestModel.transform(hashed_df_validation).\\\n",
    "    select(F.expr('float(prediction = HasDetections)').alias('correct')).\\\n",
    "    select(F.avg('correct')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.mllib.tree import GradientBoostedTrees, GradientBoostedTreesModel\n",
    "\n",
    "hashed_df_train = hashed_df_train.withColumnRenamed('HasDetections', 'label')\n",
    "hasher_cols = [col for col in chi_keep_cols if col not in ['MachineIdentifier','HasDetections']]\n",
    "\n",
    "# transformers\n",
    "hasher_cols = [col for col in chi_keep_cols if col not in ['MachineIdentifier','HasDetections']]\n",
    "hasher = feature.FeatureHasher(inputCols=hasher_cols,outputCol='hash_features', categoricalCols=hasher_cols)\n",
    "gb = GradientBoostedTrees.trainClassifier(trainingData, categoricalFeaturesInfo={}, numIterations=3)\n",
    "\n",
    "_evaluator = BinaryClassificationEvaluator(labelCol='label', metricName='areaUnderROC')\n",
    "\n",
    "paramGrid = (ParamGridBuilder() \n",
    "                .addGrid(hasher.numFeatures, [256, 512]) \\\n",
    "                .addGrid(gb.numIterations, [100,150]) \\ \n",
    "                .build()\n",
    "            )\n",
    "\n",
    "pipeline = Pipeline(stages=[\n",
    "    hasher,\n",
    "    gb\n",
    "])\n",
    "\n",
    "tvs = TrainValidationSplit(estimator=pipeline,\n",
    "                           estimatorParamMaps=paramGrid,\n",
    "                           evaluator=_evaluator,\n",
    "                           trainRatio=0.8)\n",
    "\n",
    "rf_model = tvs.fit(hashed_df_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.withColumnRenamed('HasDetections', 'label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# grid search works, but is too computationaly expensive for local dev--need to deploy in a cloud environment\n",
    "\n",
    "hasher_cols = [col for col in chi_keep_cols if col not in ['MachineIdentifier','HasDetections']]\n",
    "hasher = feature.FeatureHasher(inputCols=hasher_cols,outputCol='hash_features', categoricalCols=hasher_cols)\n",
    "rf = RandomForestClassifier(featuresCol='hash_features', labelCol='label', featureSubsetStrategy='sqrt', impurity='gini', seed=0)\n",
    "\n",
    "rf_pipeline = Pipeline(stages=[\n",
    "    hasher,\n",
    "    rf\n",
    "])\n",
    "\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(hasher.numFeatures, [64, 128]) \\\n",
    "    .addGrid(rf.numTrees, [10, 50, 100]) \\\n",
    "    .addGrid(rf.maxDepth, [5, 10, 15]) \\\n",
    "    .build()\n",
    "\n",
    "crossval = CrossValidator(estimator=rf_pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=BinaryClassificationEvaluator(),\n",
    "                          numFolds=3)\n",
    "\n",
    "cvModel = crossval.fit(df_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* crossval object will gridsearch over parameters specified in paramGrid. 54 models (2 * 3 * 3 * 3 folds) will be generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashed_df_test = feature_hasher(df_test, test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashed_df_test.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "hashed_df_test.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_test_results = rf_pipeline.transform(hashed_df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_submit = rf_test_results.select('MachineIdentifier', 'prediction').withColumnRenamed('prediction','HasDetections')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_submit.persist(StorageLevel.MEMORY_ONLY)\n",
    "rf_submit.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (rf_submit\n",
    "#  .coalesce(1) \n",
    "#  .write.format('csv')\n",
    "#  .mode('overwrite')\n",
    "#  .save('hdfs://namenode:9000/data/rf_chi_features_submit.csv')\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./ms_kaggle.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
