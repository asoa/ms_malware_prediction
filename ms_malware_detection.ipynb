{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.conf import SparkConf\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.ml import feature, evaluation, Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = (SparkConf()\n",
    "            .setAppName('ms_malware_pred')\n",
    "            .setMaster('spark://spark-master:7077')\n",
    "       )\n",
    "conf.set(\"spark.executor.cores\", \"2\")\n",
    "conf.set(\"spark.executor.memory\", \"10g\")\n",
    "conf.set(\"spark.driver.maxResultSize\", \"0\")\n",
    "conf.set(\"spark.default.parallelism\", \"6\")  # default number of cores in application\n",
    "conf.set(\"spark.driver.memory\", \"6g\") \n",
    "conf.set(\"spark.ui.showConsoleProgress\", \"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext.getConf().getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[MachineIdentifier: string, ProductName: string, EngineVersion: string, AppVersion: string, AvSigVersion: string, IsBeta: int, RtpStateBitfield: int, IsSxsPassiveMode: int, DefaultBrowsersIdentifier: int, AVProductStatesIdentifier: int, AVProductsInstalled: int, AVProductsEnabled: int, HasTpm: int, CountryIdentifier: int, CityIdentifier: int, OrganizationIdentifier: int, GeoNameIdentifier: int, LocaleEnglishNameIdentifier: int, Platform: string, Processor: string, OsVer: string, OsBuild: int, OsSuite: int, OsPlatformSubRelease: string, OsBuildLab: string, SkuEdition: string, IsProtected: int, AutoSampleOptIn: int, PuaMode: string, SMode: int, IeVerIdentifier: int, SmartScreen: string, Firewall: int, UacLuaenable: int, Census_MDC2FormFactor: string, Census_DeviceFamily: string, Census_OEMNameIdentifier: int, Census_OEMModelIdentifier: int, Census_ProcessorCoreCount: int, Census_ProcessorManufacturerIdentifier: int, Census_ProcessorModelIdentifier: int, Census_ProcessorClass: string, Census_PrimaryDiskTotalCapacity: decimal(13,0), Census_PrimaryDiskTypeName: string, Census_SystemVolumeTotalCapacity: int, Census_HasOpticalDiskDrive: int, Census_TotalPhysicalRAM: int, Census_ChassisTypeName: string, Census_InternalPrimaryDiagonalDisplaySizeInInches: double, Census_InternalPrimaryDisplayResolutionHorizontal: int, Census_InternalPrimaryDisplayResolutionVertical: int, Census_PowerPlatformRoleName: string, Census_InternalBatteryType: string, Census_InternalBatteryNumberOfCharges: decimal(20,0), Census_OSVersion: string, Census_OSArchitecture: string, Census_OSBranch: string, Census_OSBuildNumber: int, Census_OSBuildRevision: int, Census_OSEdition: string, Census_OSSkuName: string, Census_OSInstallTypeName: string, Census_OSInstallLanguageIdentifier: int, Census_OSUILocaleIdentifier: int, Census_OSWUAutoUpdateOptionsName: string, Census_IsPortableOperatingSystem: int, Census_GenuineStateName: string, Census_ActivationChannel: string, Census_IsFlightingInternal: int, Census_IsFlightsDisabled: int, Census_FlightRing: string, Census_ThresholdOptIn: int, Census_FirmwareManufacturerIdentifier: int, Census_FirmwareVersionIdentifier: int, Census_IsSecureBootEnabled: int, Census_IsWIMBootEnabled: int, Census_IsVirtualDevice: int, Census_IsTouchEnabled: int, Census_IsPenCapable: int, Census_IsAlwaysOnAlwaysConnectedCapable: int, Wdft_IsGamer: int, Wdft_RegionIdentifier: int, HasDetections: int]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = (spark.read.format('csv')\n",
    "      .option('inferSchema', 'true')\n",
    "      .option('header', 'true')\n",
    "      .option('quote', '\\\"')\n",
    "      .option('escape', '\\\"')\n",
    "      .load('hdfs://namenode:9000/data/train.csv')\n",
    "     )\n",
    "df.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# added 'MachineIdentifier' in numeric list to enable common join \n",
    "def get_num_cols(_df):\n",
    "    numeric_cols = [x[0] for x in _df.dtypes if x[1] in ['int', 'double']]\n",
    "    return numeric_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_string_cols(_df):\n",
    "    string_cols = [x[0] for x in _df.dtypes if x[1] == 'string']\n",
    "    return string_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_null_count(_df, _col):\n",
    "    # return count of null values in _col\n",
    "    df1 = _df.select(F.count(F.when(F.isnull(_col), _col)).alias('null_count'))\n",
    "    df2 = _df.groupby(_col).count().sort(F.desc('count'))\n",
    "    df1.show()\n",
    "    df2.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summ_stats(_col):\n",
    "    # return summary stats (e.g. min, max, mean, quantile for _col)\n",
    "    df.describe(_col).show()\n",
    "    print(df.approxQuantile(_col, [.25,.5,.75], 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_hist_bar(_col):\n",
    "    pdf = (df.groupby(_col).count().sort(F.desc(_col))).toPandas()\n",
    "    d = dict(df.dtypes)\n",
    "    if d[_col] == 'string' or d[_col] == 'int':\n",
    "        pdf.plot(kind='bar', x=_col, y='count')\n",
    "    else:\n",
    "        pdf.plot(kind='hist', x=_col, y='count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handle missing values\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert decimal dtypes to double"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Census_PrimaryDiskTotalCapacity', 'Census_InternalBatteryNumberOfCharges']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decimal_dtypes = [col[0] for col in df.dtypes if 'decimal' in col[1]]\n",
    "decimal_dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method withColumn in module pyspark.sql.dataframe:\n",
      "\n",
      "withColumn(colName, col) method of pyspark.sql.dataframe.DataFrame instance\n",
      "    Returns a new :class:`DataFrame` by adding a column or replacing the\n",
      "    existing column that has the same name.\n",
      "    \n",
      "    The column expression must be an expression over this :class:`DataFrame`; attempting to add\n",
      "    a column from some other :class:`DataFrame` will raise an error.\n",
      "    \n",
      "    :param colName: string, name of the new column.\n",
      "    :param col: a :class:`Column` expression for the new column.\n",
      "    \n",
      "    >>> df.withColumn('age2', df.age + 2).collect()\n",
      "    [Row(age=2, name='Alice', age2=4), Row(age=5, name='Bob', age2=7)]\n",
      "    \n",
      "    .. versionadded:: 1.3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(df.withColumn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ).withColumn(\n",
    "#        'class_label', (col('class_label').cast('int'))\n",
    "df = (df.select('*')\n",
    "        .withColumn('Census_PrimaryDiskTotalCapacity', F.col('Census_PrimaryDiskTotalCapacity').cast('double'))\n",
    "        .withColumn('Census_InternalBatteryNumberOfCharges', F.col('Census_InternalBatteryNumberOfCharges').cast('double'))\n",
    "     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop columns with high occurence of null values > 70%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "_df = df.select(\n",
    "    # get count of null values for each column\n",
    "    [(F.count(F.when(F.isnull(c), c))).alias(c) for c in df.columns]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create pandas dataframe: values are null counts of each column\n",
    "pdf = pd.DataFrame([_df.select(col).first() for col in _df.columns], _df.columns, columns=['count_null_values'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf = pdf.sort_values('count_null_values', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rows = df.count()\n",
    "pdf['null_ratio'] = pdf.count_null_values/num_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count_null_values</th>\n",
       "      <th>null_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>PuaMode</th>\n",
       "      <td>8919174</td>\n",
       "      <td>0.999741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Census_ProcessorClass</th>\n",
       "      <td>8884852</td>\n",
       "      <td>0.995894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DefaultBrowsersIdentifier</th>\n",
       "      <td>8488045</td>\n",
       "      <td>0.951416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Census_IsFlightingInternal</th>\n",
       "      <td>7408759</td>\n",
       "      <td>0.830440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Census_InternalBatteryType</th>\n",
       "      <td>6338414</td>\n",
       "      <td>0.710466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Census_ThresholdOptIn</th>\n",
       "      <td>5667325</td>\n",
       "      <td>0.635245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Census_IsWIMBootEnabled</th>\n",
       "      <td>5659703</td>\n",
       "      <td>0.634390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SmartScreen</th>\n",
       "      <td>3177011</td>\n",
       "      <td>0.356108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OrganizationIdentifier</th>\n",
       "      <td>2751518</td>\n",
       "      <td>0.308415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SMode</th>\n",
       "      <td>537759</td>\n",
       "      <td>0.060277</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            count_null_values  null_ratio\n",
       "PuaMode                               8919174    0.999741\n",
       "Census_ProcessorClass                 8884852    0.995894\n",
       "DefaultBrowsersIdentifier             8488045    0.951416\n",
       "Census_IsFlightingInternal            7408759    0.830440\n",
       "Census_InternalBatteryType            6338414    0.710466\n",
       "Census_ThresholdOptIn                 5667325    0.635245\n",
       "Census_IsWIMBootEnabled               5659703    0.634390\n",
       "SmartScreen                           3177011    0.356108\n",
       "OrganizationIdentifier                2751518    0.308415\n",
       "SMode                                  537759    0.060277"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PuaMode',\n",
       " 'Census_ProcessorClass',\n",
       " 'DefaultBrowsersIdentifier',\n",
       " 'Census_IsFlightingInternal',\n",
       " 'Census_InternalBatteryType']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop_cols = ['PuaMode','Census_ProcessorClass','DefaultBrowsersIdentifier','Census_IsFlightingInternal','Census_InternalBatteryType','Census_ThresholdOptIn','Census_IsWIMBootEnabled']\n",
    "drop_cols = list(pdf[(pdf['null_ratio'] > .70)].index.values)\n",
    "drop_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(*drop_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Impute remaining missing values to -1 for numeric and '-1' for string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_cols = get_num_cols(df)\n",
    "string_cols = get_string_cols(df)\n",
    "df = df.fillna(-1, numeric_cols)\n",
    "df = df.fillna('-1', string_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split cols into numeric and string groups\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_cols = get_num_cols(df)\n",
    "numeric_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_cols = get_string_cols(df)\n",
    "string_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get skewness of columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df_skew = df.select(\n",
    "   [F.skewness(_col).alias(_col) for _col in df.columns]\n",
    ")\n",
    "pdf_skew = pd.DataFrame([df_skew.select(_col).first() for _col in df_skew.columns], df_skew.columns, columns=['skewness'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('Census_PrimaryDiskTotalCapacity').count().sort(F.desc('count')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_skew.sort_values('skewness', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop skewed columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skew_drop_cols = pdf_skew[pdf_skew['skewness'] > 100]\n",
    "skew_drop_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(skew_drop_cols.index.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# skew_drop_cols = ['IsBeta','AutoSampleOptIn','UacLuaenable','Census_IsFlightingInternal','Census_IsFlightsDisabled','Census_IsWIMBootEnabled']\n",
    "# df = df.drop(*skew_drop_cols)\n",
    "df = df.drop(*list(skew_drop_cols.index.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_num_cols(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_string_cols(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze distribution of columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hist_bar('Census_OSSkuName')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('Census_IsVirtualDevice').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hist_bar('Census_IsVirtualDevice')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation Analysis\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for _col in numeric_cols:\n",
    "    df = df.withColumn(_col, F.col(_col).cast('float'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.stat import Summarizer\n",
    "help(Summarizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import QuantileDiscretizer\n",
    "\n",
    "data = [(0, 18.0), (1, 19.0), (2, 8.0), (3, 5.0), (4, 2.2)]\n",
    "_df1 = spark.createDataFrame(data, [\"id\", \"hour\"])\n",
    "\n",
    "discretizer = QuantileDiscretizer(numBuckets=4, inputCol=\"Census_InternalPrimaryDiagonalDisplaySizeInInches\", outputCol=\"result\")\n",
    "\n",
    "result = discretizer.fit(df).transform(df)\n",
    "result.select('result').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summ_stats('Census_InternalPrimaryDiagonalDisplaySizeInInches')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.approxQuantile('Census_InternalPrimaryDiagonalDisplaySizeInInches',[.25,.5,.75], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [_col for _col in numeric_cols if _col != 'HasDetections']\n",
    "output_cols = [f'{_col}_vec' for _col in cols]\n",
    "one_hot_trans = feature.OneHotEncoderEstimator(inputCols=cols, outputCols=output_cols).fit(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discretize  numeric columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(F.skewness('SMode')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_dimensional_cols = [(_col, df.select(_col).distinct().count()) for _col in numeric_cols]\n",
    "high_dimensional_cols = [_col for _col in high_dimensional_cols if _col[1] > 50]\n",
    "high_dimensional_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _col in numeric_cols:\n",
    "    print(_col, df.select(_col).distinct().count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_host_trans.fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_pipeline = Pipeline(stages=[\n",
    "    one_host_trans,\n",
    "    feature.Vec(inputCols=['one_hot_vec'], outputCols=['features'])\n",
    "]).fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.stat import ChiSquareTest\n",
    "# corr_vectors = [_col for _col in df.columns if \"_vec\" in _col]\n",
    "cols = [_col for _col in numeric_cols if _col != 'HasDetections']\n",
    "va = feature.VectorAssembler(inputCols=cols, outputCol='features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_df = va.transform(df).select('MachineIdentifier','features', 'HasDetections')\n",
    "corr_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.stat import ChiSquareTest\n",
    "\n",
    "r = ChiSquareTest.test(corr_df, 'features', 'HasDetections')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.stat import ChiSquareTest\n",
    "\n",
    "data = [(0.0, Vectors.dense(0.5, 10.0)),\n",
    "        (0.0, Vectors.dense(1.5, 20.0)),\n",
    "        (1.0, Vectors.dense(1.5, 30.0)),\n",
    "        (0.0, Vectors.dense(3.5, 30.0)),\n",
    "        (0.0, Vectors.dense(3.5, 40.0)),\n",
    "        (1.0, Vectors.dense(3.5, 40.0))]\n",
    "df = spark.createDataFrame(data, [\"label\", \"features\"])\n",
    "\n",
    "r = ChiSquareTest.test(df, \"features\", \"label\").head()\n",
    "print(\"pValues: \" + str(r.pValues))\n",
    "print(\"degreesOfFreedom: \" + str(r.degreesOfFreedom))\n",
    "print(\"statistics: \" + str(r.statistics))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO\n",
    "* correlation analysis\n",
    "* discretize numeric colums\n",
    "* discretize numeric fields using quartiles?\n",
    "* handle null values (drop/replace)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO create new dataframe with string indexed (StringIndexer) column of string columns with factors < 15\n",
    "### Factor columns will be vectorized and fed as input for model training induction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.stat import Correlation\n",
    "v = VectorAssembler(inputCols=['RtpStateBitfield', 'Census_OSInstallLanguageIdentifier'], outputCol='features').transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.stat import Correlation\n",
    "\n",
    "data = [(Vectors.sparse(4, [(0, 1.0), (3, -2.0)]),),\n",
    "        (Vectors.dense([4.0, 5.0, 0.0, 3.0]),),\n",
    "        (Vectors.dense([6.0, 7.0, 0.0, 8.0]),),\n",
    "        (Vectors.sparse(4, [(0, 9.0), (3, 1.0)]),)]\n",
    "df = spark.createDataFrame(data, [\"features\"])\n",
    "\n",
    "r1 = Correlation.corr(df, \"features\").head()\n",
    "print(\"Pearson correlation matrix:\\n\" + str(r1[0]))\n",
    "\n",
    "r2 = Correlation.corr(df, \"features\", \"spearman\").head()\n",
    "print(\"Spearman correlation matrix:\\n\" + str(r2[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "def _pipeline(_col):\n",
    "    string_output_col = _col + \"_index\"\n",
    "    oh_output_cols = _col + \"_vec\"\n",
    "    print(f\"Processing: {oh_output_cols}\")\n",
    "    pipeline = Pipeline(stages=[\n",
    "        StringIndexer(inputCol=_col, outputCol=string_output_col),\n",
    "        feature.OneHotEncoderEstimator(inputCols=[string_output_col], outputCols=[oh_output_cols])\n",
    "    ])\n",
    "\n",
    "    _df = pipeline.fit(df.select('MachineIdentifier',_col)).transform(df.select('MachineIdentifier',_col))\n",
    "    return _df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_one_hot_vectors = [_pipeline(_col) for _col in cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_one_hot_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_one_hot_vectors[0].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_df = [(df.join(_df,  df.MachineIdentifier == _df.MachineIdentifier)) for _df in df_with_one_hot_vectors][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "joined_df.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
